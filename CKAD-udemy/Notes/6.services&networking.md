# Services  
In this lecture, we will discuss Kubernetes Services.

Kubernetes services enable communication between various components both within the cluster and outside of the application. They help connect applications together with other applications or users. For example, imagine an application that has multiple groups of pods—one group serving a front-end to users, another group running back-end processes, and a third group connecting to an external data source. It is services that make communication possible between these groups of pods. Services also ensure that the front-end can be exposed to users, the back-end can talk to the front-end, and connectivity to external systems can be established. This design allows for loose coupling between microservices in our application.

Now, let’s look at one important use case of services: external communication. So far, we talked about how pods can communicate with each other using internal networking. But what if we want to access a web application running inside a pod from outside the cluster? Suppose the Kubernetes node has an IP address of 192.168.1.2, and my laptop on the same network has the IP 192.168.1.10. Meanwhile, the internal pod network might be in the range 10.244.0.0, and a pod could have an IP 10.244.0.2. From my laptop, I cannot directly access the pod IP because it’s in a separate network.

One option would be to SSH into the Kubernetes node and access the pod internally, but that’s not practical. What we want is to access the pod’s web server directly from our laptop. To achieve this, we need something that maps the request from the external network, through the node, and finally to the pod. This is exactly what a Kubernetes Service does. A service is an object in Kubernetes, just like pods, ReplicaSets, or deployments. One of its use cases is to listen to a port on the node and forward requests to a pod. This type of service is known as a NodePort Service.

Kubernetes actually supports three main types of services. The first is NodePort, which makes a pod’s port accessible via a port on the node. The second is ClusterIP, which creates a virtual IP inside the cluster to enable communication between different services, for example, between front-end and back-end pods. The third is a LoadBalancer Service, which provisions an external load balancer in supported cloud providers to distribute traffic across pods.

Let’s focus on NodePort in more detail. When using a NodePort service, there are three ports involved. The first is the target port, which is the port on the pod where the application (such as a web server) is running—say port 80. The second is the port on the service itself, which forwards traffic to the target port. Finally, there is the node port, which is the port exposed on the Kubernetes node that users can access externally. NodePorts must be within the valid range 30000–32767.

To create a service, we define it in a YAML file. The structure looks similar to other objects: we specify the API version (v1), kind (Service), and metadata (such as name). In the spec section, we define the service type (NodePort, ClusterIP, or LoadBalancer) and the port mappings. For example, we might set targetPort: 80, port: 80, and nodePort: 30008. Out of these, only the service port (port) is mandatory—the target port defaults to the same value if not specified, and a free node port will be assigned if one is not provided.

But here’s a key point: how does the service know which pod to forward traffic to? There could be many pods listening on port 80. To solve this, Kubernetes uses labels and selectors. We attach labels to our pods, such as app: myapp, and then configure the service with a selector that matches this label. This way, the service routes traffic to the correct set of pods.

Once the service is defined, we create it with the kubectl create -f command. Running kubectl get services shows details such as the cluster IP, node port, and service type. With this in place, we can access the application externally. For example, if the node IP is 192.168.1.2 and the node port is 30008, we can access the web application by browsing to http://192.168.1.2:30008 or using curl.

In production, we typically run multiple pods for high availability and load balancing. When multiple pods share the same labels, the service automatically discovers them and routes traffic across all of them. By default, Kubernetes uses a random load-balancing algorithm to distribute requests between pods.

Even when pods are distributed across multiple nodes, Kubernetes automatically ensures that the service works across the entire cluster. It maps the same node port across all nodes, so users can connect to any node’s IP on the same port to access the application. This makes services highly resilient and adaptive.

To summarize, a service in Kubernetes allows external and internal communication for pods, supports multiple types such as NodePort, ClusterIP, and LoadBalancer, and automatically adapts to changes in the number of pods. Once created, services require little to no manual reconfiguration, making them a core component of Kubernetes networking.

# Services - Cluster IP  
In this lecture, we will discuss the Kubernetes Service ClusterIP.

A full-stack web application typically consists of multiple pods, each hosting different parts of the application. For example, you may have a group of pods running a front-end web server, another set running a back-end server, a set hosting a key-value store like Redis, and yet another set running a persistent database such as MySQL. Naturally, the front-end servers need to communicate with the back-end servers, and the back-end servers need to connect both to the Redis pods and to the database.

So how do we establish reliable connectivity between these tiers of the application? Each pod is assigned an IP address, but these IPs are not static. Pods can be destroyed and recreated at any time, which means their IPs constantly change. This makes it impossible to rely on pod IPs for stable communication. Additionally, if one front-end pod wants to connect to the back-end, how does it know which back-end pod to use when multiple are running? Who makes that decision?

This is where Kubernetes Services come in. A service can group a set of pods together and provide a single, stable interface for other pods to access them. For instance, creating a service for all back-end pods gives them a single entry point. Any requests to this service are then forwarded randomly to one of the available back-end pods. Similarly, we can create a service for Redis, allowing the back-end to communicate with Redis reliably, without worrying about pod IPs.

This approach makes it easy to deploy and scale microservices-based applications on Kubernetes. Each layer of the application can scale up, scale down, or move across nodes without breaking communication between services. Kubernetes ensures that the service remains available, regardless of changes in the underlying pods.

Every service in Kubernetes gets a stable IP address and a DNS name inside the cluster. Other pods should always use this service name (rather than pod IPs) to access it. This type of service is known as a ClusterIP Service.

To create a ClusterIP service, we use a service definition file. As always, it begins with the standard template containing apiVersion, kind, metadata, and spec. The API version is v1, and the kind is Service. We give the service a name, such as backend.

In the spec section, we define the type and the ports. The type is set to ClusterIP, although it is worth noting that ClusterIP is the default type, so even if you omit it, Kubernetes will assume the service type is ClusterIP. Under ports, we define the targetPort, which is the port where the backend container is listening (for example, port 80), and the port, which is the port exposed on the service (also set to 80 in this case).

Finally, to connect the service to the correct set of pods, we use a selector. The selector must match the labels assigned to the backend pods. We copy the labels from the pod definition file and add them under the selector field in the service definition. This ensures that the service routes traffic only to the intended pods.

Once the definition file is ready, we create the service using the kubectl create -f command. We can then verify its status with the kubectl get services command. Other pods in the cluster can now access this backend service using either the ClusterIP or the service name.

That’s it for this lecture on the ClusterIP service. Head over to the demo, and I will see you in the next lecture.

# Network Policies  
In this lecture, we will discuss network policies in Kubernetes. Before diving into Kubernetes-specific concepts, let’s first understand some basic networking and security principles. Consider a simple application setup with a web server, an API server, and a database server. A user sends a request to the web server on port 80. The web server forwards the request to the API server on port 5000, which in turn queries the database server on port 3306. Finally, the response travels back to the user.

In this setup, there are two types of traffic: ingress and egress. Ingress refers to incoming traffic to a pod, while egress is the outgoing traffic from a pod. For instance, the web server receives ingress traffic from users and has egress traffic to the API server. The API server receives ingress from the web server and sends egress to the database. The database, in turn, receives ingress traffic from the API server. When defining ingress and egress, only the direction of traffic origin matters; responses back to the sender are not considered.

Now, let’s see how this fits into Kubernetes. In a cluster, nodes host pods and services, each with its own IP address. A prerequisite for networking in Kubernetes is that all pods should be able to communicate with each other by default, without additional routing configurations. Kubernetes sets up an "All Allow" rule by default, meaning traffic from any pod to any other pod or service is allowed.

Suppose we deploy pods for the front-end, API server, and database, and create services to enable communication. By default, all pods can communicate freely. But what if we want to restrict access, for example, preventing the front-end from talking directly to the database for security or compliance reasons? This is where network policies come into play.

A network policy is a Kubernetes object, similar to pods, replica sets, or services, that can be applied to one or more pods. It allows you to define rules that control ingress and egress traffic. For example, you can create a policy that allows the database pod to receive traffic only from the API pod on port 3306. Once applied, all other traffic to the database pod is blocked.

To apply a network policy, we use labels and selectors, the same technique used for linking replica sets or services to pods. We label the pod and then reference that label in the network policy selector. Under policyTypes, we specify whether the policy controls ingress, egress, or both. In this example, we only allow ingress traffic to the database pod. The ingress rule specifies the API pod as the source (again using labels) and the port 3306.

Putting it all together, we start with a blank object definition file containing apiVersion, kind, metadata, and spec. The apiVersion is networking.k8s.io/v1, the kind is NetworkPolicy, and we can name the policy, for example, DB-Policy. Under spec, we set the pod selector to target the database pod and define the ingress rules. Once complete, we can create the policy using the kubectl create command.

It’s important to note that ingress or egress isolation only occurs if specified in the policyTypes. In our example, since only ingress is included, only incoming traffic is isolated; egress traffic is unrestricted. To isolate egress traffic, it must also be explicitly included in the policyTypes.

Finally, network policies are enforced by the networking solution implemented in your Kubernetes cluster. Solutions like Calico, Cilium, Romana, and Weave Net support network policies. Others, such as Flannel (at the time of recording), do not. Even in clusters that do not support network policies, you can still create them, but they will not be enforced, and no error message will be generated.

That concludes this lecture on Kubernetes network policies.

# Developing network policies  
In this lecture, we will explore network policies in Kubernetes in more detail. We will use the same setup as before, with web, API, and database pods. Our primary goal is to protect the database pod so that it only allows access from the API pod on port 3306. Traffic to or from the web pod or API pod is unrestricted, but the database pod must be restricted.

By default, Kubernetes allows all traffic from all pods to all destinations. The first step is to block all traffic to and from the database pod. We do this by creating a network policy, called db-policy, and associating it with the database pod using labels and selectors. Specifically, we use the podSelector field with matchLabels to match the database pod's label, for example, role: db. This blocks all traffic to the pod.

Next, we need to allow the API pod to query the database on port 3306. To determine the type of policy required, we consider traffic from the database pod's perspective. Incoming traffic from the API pod is ingress, so we define an ingress rule. Responses from the database pod back to the API pod do not require a separate rule; Kubernetes automatically allows replies to permitted ingress traffic. Note that this rule does not allow the database pod to initiate connections to the API pod—that would require a separate egress rule.

A single network policy can specify ingress, egress, or both depending on the requirements. In this use case, only ingress is needed. Within the ingress section, we define rules using two fields: from and ports. The from field identifies the sources allowed to access the database pod, typically using a podSelector for the API pod. The ports field specifies the allowed port, in this case, TCP port 3306. This configuration blocks all other traffic to the database pod.

If there are multiple API pods in different namespaces (e.g., dev, test, prod) with the same labels, the policy must specify which namespace is allowed. We do this by adding a namespaceSelector along with the podSelector. Traffic is allowed only from pods matching the label in the specified namespace. If only the namespaceSelector is used, all pods in that namespace are allowed, while pods outside are blocked.

There may be scenarios where external servers, such as a backup server outside the cluster, need to access the database pod. In such cases, the podSelector and namespaceSelector do not apply, since the traffic originates from outside the cluster. Instead, we use ipBlock to allow traffic from a specific IP address or range, for example, 192.168.5.10.

The from section in ingress can contain multiple rules. Each rule can combine podSelector, namespaceSelector, and ipBlock. Within a single rule, multiple selectors work as an AND operation, meaning all conditions must be met. Separate rules act as an OR, meaning traffic matching any rule is allowed. Small changes in how rules are combined can have a large impact on which traffic is allowed, so careful planning is required.

Now let’s discuss egress. Suppose an agent on the database pod pushes backups to an external server. In this case, the database pod initiates the traffic, so we need an egress rule. We add egress to the policy types and define a new section under the policy. Instead of from, egress uses to to specify destinations. The same selectors—podSelector, namespaceSelector, or ipBlock—can be used. For traffic to an external server, we use ipBlock to specify the server’s CIDR, along with the port number to allow, for example, port 80.

With this configuration, we can control both ingress and egress traffic precisely, ensuring that the database pod only communicates with approved sources and destinations. By combining pod selectors, namespace selectors, and IP blocks, Kubernetes network policies provide flexible, fine-grained control over traffic within and outside the cluster.

This concludes our detailed lecture on network policies. Head over to the lab to practice creating and applying these policies yourself.

# Ingress Networking  
In this lecture, we will discuss about Ingress in Kubernetes.

We will start with a simple scenario. You are deploying an application on Kubernetes for a company that has an online store selling products. Your application would be available at, say, my-online-store.com. You build the application into a Docker image and deploy it on the Kubernetes cluster as a pod in a deployment. Your application needs a database, so you deploy a MySQL database as a pod and create a service of type ClusterIP called mysql-service to make it accessible to our application. Your application is now working.

To make the application accessible to the outside world, you create another service, this time of type NodePort, and make your application available on a high port on the nodes in the cluster. In this example, a port 38080 is allocated for the service. The users can now access your application using the URL http://IP of any of your nodes:38080. That setup works, and users are able to access the application. Whenever traffic increases, we increase the number of replicas of the pod to handle the additional traffic, and the service takes care of splitting traffic between the pods.

However, if you have deployed a production-grade application before, you know that there are many more things involved in addition to simply splitting the traffic between the pods. For example, we do not want the users to have to type in the IP address every time, so you configure your DNS server to point to the IP of the nodes. Your users can now access your application using the URL my-online-store.com and port 38080. Now, you don't want your users to have to remember port number either. However, service NodePorts can only allocate high-numbered ports, which are greater than 30,000. So you then bring in an additional layer between the DNS server and your cluster, like a proxy server that proxies requests on port 80 to port 38080 on your nodes. You then point your DNS to this server, and users can now access your application by simply visiting my-online-store.com.

Now, this is if your application is hosted on-prem in your data center. Let's take a step back and see what you could do if you were on a public cloud environment like Google Cloud Platform. In that case, instead of creating a service of type NodePort for your application, you could set it to type LoadBalancer. When you do that, Kubernetes would still do everything that it has to do for a NodePort, which is to provision a high port for the service, but in addition to that, Kubernetes also sends a request to Google Cloud Platform to provision a network load balancer for the service. On receiving the request, GCP would then automatically deploy a load balancer configured to route traffic to the service ports on all nodes and return its information to Kubernetes.

The load balancer has an external IP that can be provided to users to access the application. In this case, we set the DNS to point to this IP, and users access the application using the URL my-online-store.com. Perfect.

Your company's business grows, and you now have new services for your customers. For example, a video streaming service. Now, you want your users to be able to access your new video streaming service by going to my-online-store.com/watch. You'd like to make your old application accessible at my-online-store.com/wear. Your developers develop the new video streaming application as a completely different application as it has nothing to do with the existing one. However, to share the cluster’s resources, you deploy the new application as a separate deployment within the same cluster. You create a service called video-service of type LoadBalancer. Kubernetes provisions port 38282 for this service, and also provisions a network load balancer on the cloud. The new load balancer has a new IP. Remember, you must pay for each of these load balancers, and having many such load balancers can inversely affect your cloud bill.

So, how do you direct traffic between each of these LoadBalancers based on the URL that the user types in? You need yet another proxy or a load balancer that can redirect traffic based on URLs to the different services. Every time you introduce a new service, you have to reconfigure the load balancer. And finally, you also need to enable SSL for your applications so your users can access your application using HTTPS. Where do you configure that? It can be done at different levels, either at the application level itself or at the LoadBalancer level or at the proxy server level, but which one? And you don't want your developers to implement it in their applications as they would do it in different ways, and it's an additional burden for them to develop additional code to handle that. You want it to be configured in one place with minimal maintenance.

Now, that's a lot of different configuration, and all of these becomes difficult to manage when your application scales. It requires involving different individuals in different teams, you need to configure your firewall rules for each new service, and it's expensive as well as for each service, a new cloud-native load balancer needs to be provisioned. Wouldn't it be nice if you could manage all of that within the Kubernetes cluster and have all that configuration as just another Kubernetes definition file that lives along with the rest of your application deployment files?

That's where Ingress comes in. Ingress helps your users access your application using a single externally accessible URL that you can configure to route traffic to different services within your cluster based on the URL path, at the same time, implement SSL security as well. Think of Ingress as a Layer 7 load balancer built into the Kubernetes cluster that can be configured using native Kubernetes primitives just like any other object that we've been working with in Kubernetes.

Now, remember, even with Ingress, you still need to expose it to make it accessible outside the cluster, so you still have to either publish it as a NodePort or with a cloud-native load balancer, but that is just a one-time configuration. Going forward, you're going to perform all your load balancing of SSL and URL-based routing configurations on the Ingress controller.

So how does it work? What is it? Where is it? How can you see it, and how can you configure it, and how does it load balance? How does it implement SSL? Without Ingress, how would you do all of this? Well, I would use a reverse proxy or a load-balancing solution like nginx or HAProxy or Traefik. I would deploy them on my Kubernetes cluster and configure them to route traffic to other services. The configuration involves defining URL routes, configuring SSL certificates, et cetera.

Ingress is implemented by Kubernetes in kind of the same way. You first deploy a supported solution, which happens to be any of these listed here, and then specify a set of rules to configure Ingress. The solution you deploy is called an Ingress controller, and the set of rules you configure are called Ingress resources. Ingress resources are created using definition files like the ones we have been using to create pods, deployments, and services earlier in this course.

Now, remember, a Kubernetes cluster does not come with an Ingress controller by default. If you set up a cluster following the demos in this course, you won't have an Ingress controller built into it. So, if you simply create Ingress resources and expect them to work, they won't.

Let's look at each of these in a bit more detail. As I mentioned, you do not have an Ingress controller on Kubernetes by default, so you must deploy one. What do you deploy? There are a number of solutions available for Ingress, a few of them being GCE, which is Google's Layer 7 HTTP load balancer, nginx, Contour, HAProxy, Traefik, and Istio. Out of these, GCE and nginx are currently being supported and maintained by the Kubernetes project, and in this lecture, we will use nginx as an example.

These Ingress controllers are not just another load balancer or nginx server. The load balancer components are just a part of it. Ingress controllers have additional intelligence built into them to monitor the Kubernetes cluster for new definitions or Ingress resources and configure the nginx server accordingly.

An nginx controller is deployed as just another deployment in Kubernetes. So, we start with a deployment definition file named nginx-ingress-controller with one replica and a simple pod definition template. We will label it nginx-ingress, and the image used is nginx-ingress-controller with the right version. This is a special build of nginx built specifically to be used as an Ingress controller in Kubernetes, so it has its own set of requirements.

Within the image, the nginx program is stored at location /nginx-ingress-controller, so you must pass that as the command to start the nginx controller service. If you have worked with nginx before, you know that it has a set of configuration options such as the path to store the logs, the keep alive threshold, SSL settings, session timeout, et cetera. In order to decouple this configuration data from the nginx controller image, you must create a ConfigMap object and pass that in.

Now, remember, the ConfigMap object need not have any entries at this point. A blank object will do, but creating one makes it easy for you to modify a configuration setting in the future. You will just have to add it into this ConfigMap and not have to worry about modifying the nginx configuration files.

You must also pass in two environment variables that carry the pod's name and namespace it is deployed to. The nginx service requires these to read the configuration data from within the pod. And finally, specify the ports used by the Ingress controller, which happens to be 80 and 443.

We then need a service to expose the Ingress controller to the external world, so we create a service of type NodePort with the nginx-ingress label selector to link the service to the deployment. As mentioned before, the Ingress controllers have additional intelligence built into them to monitor the Kubernetes cluster for Ingress resources and configure the underlying nginx server when something has changed. But for the Ingress controller to do this, it requires a service account with the right set of permissions. For that, we create a service account with the correct rules and rule bindings.

So, to summarize, with a deployment of the nginx-ingress image, a service to expose it, a ConfigMap to feed nginx configuration data, and a service account with the right permissions to access all of these objects, we should be ready with an Ingress controller in its simplest form.

Now, onto the next part of creating Ingress resources. An Ingress resource is a set of rules and configurations applied on the Ingress controller. You can configure rules to, say, simply forward all incoming traffic to a single application or route traffic to different applications based on the URL. So if the user goes to my-online-store.com/wear, then route to one app, or if the user visits the watch URL, then route the user to the video app, or you could route user based on the domain name itself. For example, if the user visits wear.my-online-store.com, then route the user to the wear app, or else route the user to the video app.

Let us look at how to configure these in a bit more detail. The Ingress resource is created with a Kubernetes definition file, in this case, Ingress-wear.yaml. As with any other object, we have apiVersion, kind, metadata, and spec. The apiVersion is extensions/v1beta1, kind is Ingress, and we will name it ingress-wear, and under spec, we have backend. Now remember that the apiVersion for Ingress is extension/v1beta1 as of this recording, but this is expected to change with newer releases of Kubernetes. So when you are deploying Ingress, always remember to refer to the Kubernetes documentation to know exactly the right apiVersion for that release of Kubernetes.

So the traffic is, of course, routed to the application services and not pods directly. The backend section defines where the traffic will be routed to. So, if it's a single backend, then you don't really have any rules. You can simply specify the service name and port of the backend wear-service. Create the Ingress resource by running the kubectl create command. View the created Ingress by running the kubectl get ingress command. The new Ingress is now created and routes all incoming traffic directly to the wear-service.

You use rules when you want to route traffic based on different conditions. For example, you create one rule for traffic originating from each domain or hostname. That means when users reach your cluster using the domain name my-online-store.com, you can handle that traffic using rule one. When users reach your cluster using domain name wear.my-online-store.com, you can handle that traffic using a separate rule, rule two. Use rule three to handle traffic from watch.my-online-store.com and say, use the fourth rule to handle everything else. And just in case you didn't know, you could get different domain names to reach your cluster by adding multiple DNS entries, all pointing to the same Ingress controller service on your Kubernetes cluster.

Now, within each rule, you can handle different paths. For example, within rule one, you can handle the wear path to route that traffic to the clothes application and a watch path to route traffic to the video streaming application, and a third path that routes anything other than the first 2 to a 404 Not Found page. Similarly, the second rule handles all traffic from wear.my-online-store.com. You can have path definition within this rule to route traffic based on different paths. For example, say you have different applications and services within the apparel section for shopping or returns or support. When a user goes to wear.my-online-store.com, by default, they reach the shopping page. But if they go to exchange or support URL, they reach a different backend service. The same goes for rule three where you route traffic to watch.my-online-store.com to the video streaming application, but you can have additional paths in it such as movies or TV. And finally, anything other than the ones listed here will go to the fourth rule that would simply show a 404 Not Found error page.

So remember, you have rules at the top for each host or domain name, and within each rule, you have different paths to route traffic based on the URL.

Now, let's look at how we configure Ingress resources in Kubernetes. We will start where we left off. We start with a similar definition file. This time, under spec, we start with a set of rules. Now, our requirement here is to handle all traffic coming into my-online-store.com and route them based on the URL path. So we just need a single URL for this since we are only traffic to a single domain name, which is my-online-store.com in this case.

Under rules, we have one item, which is an HTTP rule in which we specify different paths. So, paths is an array of multiple items, one path for each URL. Then we move the backend we used in the first example under the first path. The backend specification remains the same. It has a service name and service port. Similarly, we create a similar backend entry to the second URL path for the watch-service to route all traffic coming in through the watch URL to the watch-service. Create the Ingress resource using the kubectl create command. Once created, view additional details about the Ingress resource by running the kubectl describe ingress command. You now see two backend URLs under the rules and the backend service they are pointing to, just as we created it.

Now, if you look closely in the output of this command, you see that there is something about a default backend. Hmm, what might that be? If a user tries to access a URL that does not match any of these rules, then the user is directed to the service specified as the default backend. In this case, it happens to be a service named default-http-backend. So, you must remember to deploy as such a service. Back in your application, say a user visits the URL my-online-store.com/listen or eat, and you don't have an audio-streaming or a food delivery service, you might want to show them a nice message. You can do this by configuring a default backend service to display this 404 Not Found error page.

The third type of configuration is using domain names or hostnames. We start by creating a similar definition file for Ingress. Now that we have two domain names, we create two rules, one for each domain. To split traffic by domain name, we use the host field. The host field in each rule matches the specified value with the domain name used in the request URL and routes traffic to the appropriate backend. In this case, note that we only have a single backend path for each rule, which is fine. All traffic from these domain names will be routed to the appropriate backend, irrespective of the URL path used. YYou can still have multiple path specifications
in each of these to handle different URL paths.Now, let's compare the two.Splitting traffic by URL had just one rule,
and we split the traffic with two paths.To display traffic by hostname,we used two rules and one path specification in each rule.

# Article: Ingress  
As we already discussed Ingress in our previous lecture. Here is an update.
In this article, we will see what changes have been made in previous and current versions in Ingress.
Like in apiVersion, serviceName and servicePort etc.
1. 
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-wear-watch
spec:
  rules:
  - http:
      paths:
      - path: /wear
        backend:
          serviceName: wear-service
          servicePort: 80
      - path: /watch
        backend:
          serviceName: watch-service
          servicePort: 80
2. 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wear-watch
spec:
  rules:
  - http:
      paths:
      - path: /wear
        pathType: Prefix
        backend:
          service:
            name: wear-service
            port:
              number: 80
      - path: /watch
        pathType: Prefix
        backend:
          service:
            name: watch-service
            port:
              number: 80

Now, in k8s version 1.20+ we can create an Ingress resource from the imperative way like this:-
Format - kubectl create ingress <ingress-name> --rule="host/path=service:port"
Example - kubectl create ingress ingress-test --rule="wear.my-online-store.com/wear*=wear-service:80"
Find more information and examples in the below reference link:-
https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-ingress-em-

References:-
https://kubernetes.io/docs/concepts/services-networking/ingress
https://kubernetes.io/docs/concepts/services-networking/ingress/#path-types

# FAQ - What is the rewrite-target option?  
Different ingress controllers have different options that can be used to customise the way it works. NGINX Ingress controller has many options that can be seen here. I would like to explain one such option that we will use in our labs. The Rewrite target option.

Our watch app displays the video streaming webpage at http://<watch-service>:<port>/
Our wear app displays the apparel webpage at http://<wear-service>:<port>/

We must configure Ingress to achieve the below. When user visits the URL on the left, his request should be forwarded internally to the URL on the right. Note that the /watch and /wear URL path are what we configure on the ingress controller so we can forwarded users to the appropriate application in the backend. The applications don't have this URL/Path configured on them:

http://<ingress-service>:<ingress-port>/watch --> http://<watch-service>:<port>/
http://<ingress-service>:<ingress-port>/wear --> http://<wear-service>:<port>/

Without the rewrite-target option, this is what would happen:
http://<ingress-service>:<ingress-port>/watch --> http://<watch-service>:<port>/watch
http://<ingress-service>:<ingress-port>/wear --> http://<wear-service>:<port>/wear

Notice watch and wear at the end of the target URLs. The target applications are not configured with /watch or /wear paths. They are different applications built specifically for their purpose, so they don't expect /watch or /wear in the URLs. And as such the requests would fail and throw a 404 not found error.

To fix that we want to "ReWrite" the URL when the request is passed on to the watch or wear applications. We don't want to pass in the same path that user typed in. So we specify the rewrite-target option. This rewrites the URL by replacing whatever is under rules->http->paths->path which happens to be /pay in this case with the value in rewrite-target. This works just like a search and replace function.
For example: replace(path, rewrite-target)
* In our case: replace("/path","/")

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test-ingress
  namespace: critical-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /pay
        backend:
          serviceName: pay-service
          servicePort: 8282

* In another example given here, this could also be:
replace("/something(/|$)(.*)", "/$2")

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
  name: rewrite
  namespace: default
spec:
  rules:
  - host: rewrite.bar.com
    http:
      paths:
      - backend:
          serviceName: http-svc
          servicePort: 80
        path: /something(/|$)(.*)



