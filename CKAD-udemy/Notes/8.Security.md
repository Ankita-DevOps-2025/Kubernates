# Authentication, Authorization and Admission Control  
In this lecture, we look at the security primitives in Kubernetes. Kubernetes, being the go-to platform for hosting production-grade applications, makes security a prime concern. Here, we will review the various security mechanisms in Kubernetes at a high level before diving deeper into each topic in the upcoming lectures.

Letâ€™s begin with the hosts that form the cluster itself. All access to these hosts must be secured. Root access should be disabled, password-based authentication should be turned off, and only SSH key-based authentication should be enabled. Additionally, any other measures necessary to secure your physical or virtual infrastructure that hosts Kubernetes should be implemented. If the host itself is compromised, the entire cluster is at risk.

Our primary focus in this lecture is Kubernetes-related security. The kube-apiserver is at the center of all operations within Kubernetes. Users interact with it via the kubectl utility or by accessing the API directly. The API server controls almost every operation in the cluster, making it the first line of defense. When securing access to the API server, we need to make two types of decisions: who can access the cluster and what they are allowed to do.

Authentication mechanisms define who can access the API server. There are several ways to authenticate, including user IDs and passwords stored in static files, tokens, certificates, or integrations with external authentication providers like LDAP. For machines and automated components, Kubernetes provides service accounts, which we will explore in more detail in upcoming lectures.

Once users or machines gain access to the cluster, authorization mechanisms determine what actions they can perform. Authorization is typically implemented using Role-Based Access Control (RBAC), where users are associated with groups that have specific permissions. Other authorization modules include Attribute-Based Access Control (ABAC), node authorizers, and webhooks, which we will also cover in more detail later.

All communication between cluster componentsâ€”such as the etcd cluster, kube-controller-manager, scheduler, and API server, as well as components running on worker nodes like the kubelet and kube-proxyâ€”is secured using TLS encryption. We will have a dedicated section to discuss and practice setting up certificates to secure this communication.

Finally, what about communication between applications within the cluster? By default, all ports can access all other ports within the cluster. However, you can restrict access between applications using network policies. This topic will be explored in detail in the network policy section.

In summary, this lecture provided a high-level overview of Kubernetes security primitives, including authentication, authorization, communication security, and network isolation. We will explore each of these areas in much more detail in the upcoming lectures.

# Authentication  
Hello, and welcome to this lecture on authentication in a Kubernetes cluster. As we have seen, a Kubernetes cluster consists of multiple nodesâ€”physical or virtualâ€”and various components that work together. There are different types of users accessing the cluster: administrators who perform administrative tasks, developers who deploy or test applications, end users who access applications deployed on the cluster, and third-party applications that integrate with the cluster.

Throughout this section, we will focus on securing the cluster by securing communication between internal components and managing access to the cluster through authentication and authorization mechanisms. In this lecture, our focus is specifically on authenticationâ€”securing access to the Kubernetes cluster.

We discussed the different users who may access the cluster. The security of end users accessing deployed applications is handled by the applications themselves, so we will exclude them from this discussion. Our focus is on users who need access to the Kubernetes cluster for administrative purposes, which includes humans (administrators and developers) and robots (processes, services, or applications requiring cluster access).

Kubernetes does not natively manage user accounts. Instead, it relies on external sources such as files with user details, certificates, or third-party identity services like LDAP. This means you cannot directly create users or view a list of users within Kubernetes. However, service accounts can be managed by Kubernetes, and you can create and manage them via the Kubernetes API. We have a dedicated section on service accounts for further practice.

All user access in Kubernetes is managed through the kube-apiserver. Whether you are using kubectl or accessing the API directly, all requests go through the API server, which authenticates each request before processing it.

There are several authentication mechanisms available in Kubernetes. You can authenticate users using:
    Static password files with a list of usernames and passwords
    Static token files with tokens assigned to users
    Certificates for user or machine authentication
    Third-party authentication protocols such as LDAP or Kerberos

Letâ€™s start with static password and token files, as they are the simplest to understand. You can create a CSV file listing users, their passwords, and user IDs. This file is then passed as an option to the kube-apiserver. In setups using kubeadm, you would modify the kube-apiserver pod definition file, and kubeadm automatically restarts the API server after changes. To authenticate using these basic credentials, you can pass the username and password with a curl request. Optionally, the CSV file can include a group column to assign users to specific groups.

Similarly, instead of a static password file, you can use a static token file. In this file, each user is assigned a token instead of a password. The token file is specified using the --token-auth-file option on the kube-apiserver. To authenticate, the token is provided as a bearer token in API requests.

It is important to note that storing usernames, passwords, or tokens in cleartext files is not recommended in production, as it is insecure. However, this approach is helpful for understanding the basics of Kubernetes authentication. When testing in a kubeadm setup, ensure you consider volume mounts to pass the authentication files correctly. Finally, remember to configure authorization for these new users, which we will discuss in detail later in the course.

# Important Updates  
Setup basic authentication on Kubernetes (Deprecated in 1.19)
Note: This is not recommended in a production environment. This is only for learning purposes. Also note that this approach is deprecated in Kubernetes version 1.19 and is no longer available in later releases

Follow the below instructions to configure basic authentication in a kubeadm setup.
Create a file with user details locally at /tmp/users/user-details.csv
* User File Contents
password123,user1,u0001
password123,user2,u0002
password123,user3,u0003
password123,user4,u0004
password123,user5,u0005
Edit the kube-apiserver static pod configured by kubeadm to pass in the user details. The file is located at /etc/kubernetes/manifests/kube-apiserver.yaml

apiVersion: v1
kind: Pod
metadata:
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
      <content-hidden>
    image: k8s.gcr.io/kube-apiserver-amd64:v1.11.3
    name: kube-apiserver
    volumeMounts:
    - mountPath: /tmp/users
      name: usr-details
      readOnly: true
  volumes:
  - hostPath:
      path: /tmp/users
      type: DirectoryOrCreate
    name: usr-details
Modify the kube-apiserver startup options to include the basic-auth file

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --authorization-mode=Node,RBAC
      <content-hidden>
    - --basic-auth-file=/tmp/users/user-details.csv
Create the necessary roles and role bindings for these users:

---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
 
---
# This role binding allows "jane" to read pods in the "default" namespace.
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: User
  name: user1 # Name is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role #this must be Role or ClusterRole
  name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io
Once created, you may authenticate into the kube-api server using the users credentials

curl -v -k https://localhost:6443/api/v1/pods -u "user1:password123"

# KubeConfig  
Hello and welcome to this lecture. In this session, we will look at kubeconfigs in Kubernetes. So far, we have seen how to generate a certificate for a user and how a client can use the certificate and key to query the Kubernetes REST API, for example, to list pods using curl.

In this case, suppose my cluster is called my-kube-playground. We can send a curl request to the API server address while passing the certificate, key, and the CA certificate as options. The API server validates these credentials to authenticate the user.

Now, how do you do the same thing using the kubectl command? You can provide the same informationâ€”server, client key, client certificate, and certificate authorityâ€”using command-line options in kubectl. However, typing these options every time is tedious. To simplify this, Kubernetes uses a configuration file called kubeconfig. You can specify this file in the kubectl command using the --kubeconfig option.

By default, kubectl looks for a file named config under a .kube directory in the userâ€™s home directory. If the kubeconfig file is placed there, you donâ€™t need to specify the file path explicitly in each command. Thatâ€™s why most kubectl commands work without extra options.

The kubeconfig file has a specific structure in YAML format. It has three main sections: clusters, users, and contexts.
    Clusters: Define the Kubernetes clusters you need access to. You may have multiple clusters for development, testing, production, or different cloud providers.

    Users: Define user accounts that access these clusters, such as admin users, developers, or prod users. Each user may have different privileges on different clusters.

    Contexts: Link users to clusters. Contexts define which user account will be used to access which cluster. For example, a context named admin@production uses the admin account to access the production cluster, while another context may use a dev user to access a Google cluster.

It is important to note that creating a kubeconfig file does not create new users or manage their privileges. You are simply defining which existing user credentials to use when accessing which cluster, so you donâ€™t have to repeatedly specify certificates or server addresses.

In practice, the server address goes under the clusters section, the user credentials go under the users section, and a context links the two together. For example, you could create a context my-kube-admin@my-kube-playground to link the my-kube-admin user to the my-kube-playground cluster. You can repeat this procedure to add multiple clusters, users, and contexts.

Once the kubeconfig file is ready, kubectl reads it directlyâ€”there is no need to create any Kubernetes objects. You can specify which context to use as the default by setting the current-context field in the file. For example, current-context: dev-user@google tells kubectl to use the dev user on the Google cluster by default.

kubectl also provides commands to view and modify the kubeconfig file. Use kubectl config view to see the clusters, users, contexts, and current context. If you want to use a custom kubeconfig file, you can pass the --kubeconfig option on the command line. To switch contexts, use kubectl config use-context <context-name>, which updates the current context in the file.

You can also specify namespaces in the context section. This ensures that when you switch to a particular context, kubectl automatically works within a specific namespace.

Finally, regarding certificates, you can either specify the file path for client certificates and keys or embed the contents directly in the kubeconfig using base64-encoded data fields such as certificate-authority-data. This is useful for sharing kubeconfig files without distributing separate certificate files.

Head over to the practice exercises section to work with kubeconfig files and troubleshoot any issues. This hands-on practice will help solidify your understanding of managing Kubernetes access through kubeconfig.

# API Groups  
Before we head into authorization, it is necessary to understand API groups in Kubernetes. But first, what is the Kubernetes API? We have already learned about the Kube API server. Whatever operations we have done so far with the cluster, we have been interacting with the API server in one way or another, either through the kubectl utility or directly via REST.

For example, to check the cluster version, we can access the API server at the master nodeâ€™s address followed by port 6443 (the default), and the API version. It will return the cluster version. Similarly, to get a list of pods, you would access the URL /api/v1/pods. Our focus in this lecture is on understanding these APIs: the version API and the API for accessing cluster resources.

The Kubernetes API is grouped into multiple API groups based on their purpose. Some groups are for cluster APIs, some for health, metrics, and logs. The version API allows viewing the cluster version, metrics and health APIs help monitor cluster health, and logs APIs are used for integrating with third-party logging applications. In this lecture, we focus on APIs responsible for core cluster functionality.

These APIs are categorized into two main groups: the core group and named groups. The core group contains all the core functionality, such as namespaces, pods, replication controllers, events, endpoints, nodes, bindings, persistent volumes, persistent volume claims, config maps, secrets, services, etc. The named groups are more organized and are used for newer features. Named groups include apps, extensions, networking, storage, authentication, authorization, and more.

Within the apps group, for example, you have deployments, replica sets, and StatefulSets. The networking group contains network policies. The certificates group contains resources like certificate signing requests. At the top level, these are the API groups, and under each group are the resources. Each resource supports a set of actions, known as verbs, such as list, get, create, delete, update, and watch. The Kubernetes API reference page shows the API group for each object. For core group resources, the version is just v1.

You can also view API groups directly from your Kubernetes cluster. Accessing the Kube API server at port 6443 without any path lists the available API groups. Within named API groups, it shows all supported resource groups. A quick note: if you try to access the API directly using curl, you will not be allowed access except for certain APIs like version, unless you provide authentication credentials. You must authenticate to the API using certificates, passing them on the command line.

An alternate option is to start a kubectl proxy client. The kubectl proxy command launches a local proxy service on port 8001 and uses credentials and certificates from your kubeconfig file to access the cluster. This way, you do not need to pass certificates manually with curl. The proxy service forwards requests to the Kube API server and lists all available APIs at the root.

It is important to distinguish between kube-proxy and kubectl proxy. They sound similar but serve different purposes. Kube-proxy enables connectivity between pods and services across nodes in the cluster, whereas kubectl proxy is a local HTTP proxy created by the kubectl utility to access the Kube API server.

In summary, all resources in Kubernetes are grouped into different API groups. At the top level, there is the core API group and named API groups. Under named API groups, there are specific sections like apps, networking, storage, etc. Each resource within a group has a set of associated actions, known as verbs. In the next section on authorization, we will see how these API groups and resources are used to allow or deny access to users.

Well, thatâ€™s it for this lecture. I will see you in the next one.

# Authorization  
So far, we have talked about authenticationâ€”how someone can gain access to a Kubernetes cluster. We saw the different ways a human or a machine can get access to the cluster. Once they gain access, the next question is: what can they do? That is exactly what authorization defines.

First of all, why do you need authorization in your cluster? As an administrator, we are able to perform all sorts of operations, such as viewing various objects like pods, nodes, and deployments, and creating or deleting objects, including adding or removing pods or nodes. But soon, other users will access the cluster as well, such as other administrators, developers, testers, or applications like monitoring tools or continuous delivery platforms like Jenkins.

We create accounts for these users by providing usernames, passwords, tokens, signed TLS certificates, or service accounts. However, we do not want all users to have the same level of access as administrators. For example, developers may need to deploy applications but should not modify cluster configuration, such as adding or deleting nodes, storage, or networking resources. Similarly, service accounts should only have the minimum privileges required to perform their tasks. When sharing a cluster between teams or organizations, logically partitioned using namespaces, we want to restrict access so that users can only interact with resources in their assigned namespaces. This is where authorization comes into play.

Kubernetes supports different authorization mechanisms, including node authorization, attribute-based access control (ABAC), role-based access control (RBAC), and Webhook.

The node authorizer handles requests coming from kubelets on cluster nodes. Kubelets access the API server to read information about services, endpoints, nodes, and pods, and report the status of nodes. Requests coming from users with the name prefixed with system:node and part of the system:nodes group are authorized by the node authorizer, granting the privileges required for kubelet operations.

For external access to the API server, attribute-based authorization can be used. Here, a user or group is associated with a set of permissions. For example, you may allow a dev user to view, create, and delete pods by creating a policy file in JSON or YAML format and passing it to the API server. However, ABAC configurations are difficult to manage because any changes require editing the policy file manually and restarting the API server.

This is where role-based access control (RBAC) simplifies management. Instead of directly associating users or groups with permissions, you define roles with the required permissions and then associate users or groups with those roles. For example, you can create a developer role with permissions to manage deployments and pods and associate all developers with this role. Any changes to the role automatically apply to all users associated with it. RBAC provides a standardized and scalable way to manage access within a Kubernetes cluster and will be discussed in more detail in upcoming lectures.

If you want to manage authorization externally, Webhook-based authorization can be used. Tools like Open Policy Agent (OPA) can act as an external authorizer. Kubernetes sends user information and access requests to the OPA service, which evaluates the request and returns a decision. Based on this response, the user is granted or denied access.

Finally, there are two additional modes: always allow and always deny. As the names suggest, always allow permits all requests without checks, while always deny blocks all requests. These modes are configured using the --authorization-mode option on the API server. By default, the mode is always allow. You can specify a comma-separated list of multiple modes, such as Node, RBAC, Webhook. When multiple modes are configured, requests are evaluated in the order specified. A request denied by one module is passed to the next in the chain until a module grants permission, at which point no further checks are performed.

In summary, authorization in Kubernetes ensures that users and processes have the appropriate level of access to cluster resources. It allows administrators to enforce security policies, restrict access to specific namespaces, and manage permissions efficiently across multiple users and applications.

Well, thatâ€™s it for this lecture. In the upcoming lectures, we will dive deeper into RBAC and how to implement it effectively.

# Role Based Access Controls  
In this lecture, we'll look at role-based access controls (RBAC) in much more detail. So, how do we create a role? We do that by creating a Role object. A role definition file is created with the API version set to rbac.authorization.k8s.io/v1 and kind set to Role. We name the role developer since it is intended for developers.

Next, we specify rules in the role. Each rule has three sections: API groups, resources, and verbs. For core resources, the API group can be left blank, while for other groups, the group name must be specified. For example, to give developers access to pods, we list pods as the resource and assign the actions list, get, create, and delete as the verbs. Similarly, to allow developers to create config maps, another rule is added to the role. Multiple rules can be included in a single role to cover all required permissions. Once defined, the role can be created using the kubectl create role command.

The next step is to link the user to the role. This is done using a RoleBinding object, which connects a user to a role. In our example, we create a RoleBinding named dev-user-to-developer-binding. The kind is RoleBinding, and it has two sections: the subject, where user details are specified, and the roleRef, where the role details are referenced. This RoleBinding is created using the kubectl create command.

It is important to note that roles and role bindings are namespace-scoped. In this example, the dev-user gets access to pods and config maps within the default namespace. If access should be limited to a different namespace, the namespace must be specified in the metadata section of the definition file when creating the role and role binding.

To view existing roles, run kubectl get roles. To list role bindings, run kubectl get rolebindings. For more details about a role, use kubectl describe role developer to see the resources and permissions defined. Similarly, kubectl describe rolebinding <name> displays details about a specific role binding.

If a user wants to check whether they have access to a particular resource in the cluster, the kubectl auth can-i command can be used. For example, a user can check if they can create deployments or delete nodes. Administrators can impersonate another user using the --as option to test permissions without logging in as that user. For instance, if developers are not granted permissions to create deployments, the command will return no, but they will have permission to create pods in the namespace they are allowed to access.

A quick note on resource-level permissions: RBAC can also restrict access to specific resources within a namespace. For example, if a namespace has five pods, you can limit access to only the blue and orange pods by adding a resourceNames field to the role. This allows fine-grained control over access to cluster resources.

Well, thatâ€™s it for this lecture. Head over to the practice exercises section to implement and test roles, role bindings, and user permissions in Kubernetes.

# Cluster Roles  
We discussed roles and role bindings in the previous lecture. In this lecture, we will talk about ClusterRoles and ClusterRoleBindings. When we talked about roles and role bindings, we mentioned that they are namespace-scoped, meaning they are created within a namespace. If no namespace is specified, they are created in the default namespace and control access within that namespace alone.

In one of the previous lectures, we discussed namespaces and how they help in grouping or isolating resources like pods, deployments, and services. But what about other resources, such as nodes? Can you group or isolate nodes within a namespace? For example, can you say node01 is part of the dev namespace? The answer is no. Those resources are cluster-scoped and cannot be associated with any particular namespace.

Resources in Kubernetes are categorized as either namespace-scoped or cluster-scoped. Namespace-scoped resources include pods, replica sets, jobs, deployments, services, secrets, and, as we saw in the last lecture, roles and role bindings. These resources are created in the namespace you specify when creating them. If no namespace is specified, they default to the default namespace. To view, update, or delete these resources, you must always specify the correct namespace.

Cluster-scoped resources, on the other hand, do not belong to any namespace. Examples include nodes, persistent volumes, cluster roles, cluster role bindings (the focus of this lecture), certificate signing requests, and namespaces themselves. Note that this is not a comprehensive list; to see all namespace-scoped and cluster-scoped resources, you can run the command kubectl api-resources --namespaced=true for namespace-scoped resources.

In the previous lecture, we saw how to authorize a user to namespace-scoped resources using roles and role bindings. But how do we authorize users to cluster-scoped resources like nodes or persistent volumes? That is where ClusterRoles and ClusterRoleBindings come into play. ClusterRoles are similar to roles but are intended for cluster-scoped resources.

For example, a cluster admin role can be created to provide permissions to view, create, or delete nodes across the cluster. Similarly, a storage administrator role can be created to allow a user to manage persistent volumes. A cluster role is defined in a YAML file with kind: ClusterRole and includes rules for the resources and verbs, similar to a regular Role. After creating the ClusterRole, it can be applied using kubectl create -f <file>.

The next step is to link a user to the ClusterRole. This is done using a ClusterRoleBinding. The ClusterRoleBinding object connects a user to the ClusterRole. In the definition, the subjects section specifies the user details (for example, cluster-admin-user), and the roleRef section references the ClusterRole being assigned. The ClusterRoleBinding is also created using kubectl create -f <file>.

One important point is that while ClusterRoles and ClusterRoleBindings are primarily used for cluster-scoped resources, this is not a strict limitation. You can create a ClusterRole for namespace-scoped resources as well. When you do that, the user gains access to those resources across all namespaces. For instance, if a user is authorized via a ClusterRole to access pods, they will have access to all pods across the cluster, unlike a regular Role, which limits access to a specific namespace.

Kubernetes creates a number of ClusterRoles by default when the cluster is first set up. These built-in ClusterRoles provide common administrative or operational permissions. We will explore these default ClusterRoles and their usage in the practice exercises.

# Admission Controllers  
In this lecture, we will learn about admission controllers. So far, weâ€™ve been running commands from our command line using the kubectl utility to perform various operations on our Kubernetes cluster. Every time we send a request, for example, to create a pod, the request goes to the API server, and then the pod is created, with the information persisted in the etcd database.

When the request hits the API server, it goes through an authentication process, usually done through certificates. If the request is sent through kubectl, the kubeconfig file contains the certificates configured. Authentication identifies the user who sent the request and ensures that the user is valid.

After authentication, the request goes through an authorization process, which checks if the user has permission to perform the requested operation. This is typically achieved through role-based access control (RBAC). For instance, if a user has a developer role, they may be allowed to list, get, create, update, or delete pods. Requests that match these conditions are allowed to proceed; otherwise, they are rejected.

With RBAC, you can place different kinds of restrictions, such as allowing or denying specific roles from creating, listing, or deleting objects like pods, deployments, or services. You can even restrict access to specific resource names (e.g., only pods named blue or orange) or within a specific namespace. However, RBAC rules operate at the API level and define what users can do with resourcesâ€”they do not go beyond that.

This is where admission controllers come in. Admission controllers allow you to enforce policies beyond simple access control. For example, when a pod creation request comes in, you might want to ensure that images are only pulled from an internal registry, never use the latest tag, prevent containers from running as root, restrict certain capabilities, or enforce that metadata always contains labels. Admission controllers can validate requests, reject them, modify the request itself, or perform additional operations before the pod is created.
Kubernetes comes with several prebuilt admission controllers. Examples include:
    AlwaysPullImages: Ensures that images are always pulled when a pod is created.
    DefaultStorageClass: Observes PVC creation and automatically adds a default storage class if one is not specified.
    EventRateLimit: Sets a limit on the number of requests the API server can handle at a time to prevent flooding.
    NamespaceExists: Rejects requests to namespaces that do not exist.

For example, consider the NamespaceExists admission controller. If you try to create a pod in a namespace called blue that doesnâ€™t exist, the request is rejected. The request is first authenticated and authorized, then checked by the admission controllers. The NamespaceExists controller verifies the namespace exists and rejects the request if it doesnâ€™t.

There is also a NamespaceAutoProvision admission controller, which is not enabled by default. This controller can automatically create a namespace if it does not exist. To see a list of enabled admission controllers, you can run kube-apiserver -h and look for the --enable-admission-plugins flag. In a kubeadm-based setup, this must be run inside the API server pod using kubectl exec.

To add a new admission controller, update the --enable-admission-plugins flag in the API server service or manifest file. Similarly, admission controllers can be disabled using the --disable-admission-plugins flag. After updating, when a pod is provisioned in a non-existent namespace, the NamespaceAutoProvision controller will automatically create the namespace and allow the request to proceed.

Itâ€™s worth noting that the NamespaceExists and NamespaceAutoProvision admission controllers are deprecated and have been replaced by the NamespaceLifecycle admission controller. This controller ensures that requests to non-existent namespaces are rejected and protects default namespaces such as default, kube-system, and kube-public from deletion.

In summary, admission controllers provide a powerful way to enforce policies, validate requests, and even modify or create resources automatically before they are persisted. Head over to the labs to practice working with admission controllers.

# Validating and Mutating Admission Controllers  
In this lecture, we will take a closer look at the different types of admission controllers and how we can configure our own admission controller. Previously, we looked at the NamespaceExists or NamespaceLifecycle admission controller, which can validate if a namespace already exists and reject requests if it doesnâ€™t. This type of controller is known as a validating admission controller.

Another example is the DefaultStorageClass admission controller, which is enabled by default. When submitting a request to create a PVC, the request goes through authentication, authorization, and finally the admission controller. If no storage class is specified, this mutating admission controller modifies the request to add the default storage class configured in the cluster. When the PVC is created, inspecting it shows the default storage class added, even if it was not specified initially.

Thus, there are two types of admission controllers:

Mutating admission controllers, which can modify the request before it is created.

Validating admission controllers, which validate requests and allow or deny them.

Some admission controllers can perform both functions. Generally, mutating controllers are invoked first, followed by validating controllers, so that changes made by mutating controllers are considered during validation. For example, the NamespaceAutoProvisioning controller (mutating) runs before NamespaceExists (validating). If this order were reversed, requests to non-existent namespaces would always be rejected before auto-provisioning could create the namespace.

If any admission controller rejects a request, the request is rejected, and an error message is shown to the user. The built-in admission controllers are part of the Kubernetes source code, compiled, and shipped with Kubernetes. But what if we want to implement our own admission controller with custom validation or mutation logic?

Kubernetes supports external admission controllers using two special types: MutatingAdmissionWebhook and ValidatingAdmissionWebhook. These webhooks point to a server hosted either inside the cluster or externally, which runs custom logic to handle incoming requests. After a request passes through built-in controllers, it hits the configured webhook, which receives an AdmissionReview object in JSON format. This object contains details about the request, including the user, operation, object type, and object details.

The webhook server processes the request and responds with another AdmissionReview object indicating whether the request is allowed. If the allowed field is true, the request proceeds; if false, it is rejected.

To set this up, we first deploy our own webhook server, which can be built using any platform or language as long as it accepts the mutate and validate APIs and responds with the expected JSON object. For example, a Python server could have a validate call that rejects requests if the username matches the object name and a mutate call that adds the username as a label to the object.

Once developed, the webhook server can be hosted externally or deployed as a Kubernetes deployment within the cluster. If deployed inside the cluster, it requires a service for access. Next, we configure Kubernetes to use this webhook by creating a Webhook Configuration object. For validating webhooks, the API version is admissionregistration.k8s.io/v1 and the kind is ValidatingWebhookConfiguration. For mutating webhooks, it is MutatingWebhookConfiguration.

The configuration specifies the webhook name, client configuration (pointing to the webhook server), and a set of rules defining when the webhook should be invoked. If the webhook server is internal, the client config points to the service name and namespace; if external, a URL can be used. Communication between the API server and webhook server must be over TLS, so certificates and a CA bundle are configured in the client config.

Rules specify when the webhook should be invoked, including API groups, versions, operation types, and resources. For example, we may configure the webhook to run only for pod creation requests. Once this configuration is in place, every request that matches the rules is sent to the webhook service, and depending on the response, the request is allowed or rejected.

This concludes the lecture on admission webhooks. Head over to the labs to practice working with webhooks, and I will see you in the next lecture.

# API Versions  
We will now discuss API versions in Kubernetes. Previously, we talked about APIs, API groups, resources, and verbs. Now itâ€™s important to understand the concept of API versions. Every API group, such as apps, extensions, networking, etc., has different versions.

The V1 version indicates a GA (Generally Available) stable version. API groups may also have Beta or Alpha versions, such as v1beta1 or v1alpha1. These versions indicate the maturity and stability of the API.

Alpha is when an API is first developed and merged into the Kubernetes codebase. At this stage, the API may be named v1alpha1 or v1alpha2, for example. Alpha APIs are not enabled by default, may lack end-to-end tests, and could contain bugs. There is also no guarantee that they will be available in future releases. Alpha APIs are intended for expert users who want to test features and provide early feedback. As of now, the internal.apiserver.k8s.io group is in Alpha.

Once the major bugs in an Alpha API are fixed and end-to-end tests are added, it progresses to Beta. Beta APIs, like v1beta1 or v1beta2, are enabled by default and come with end-to-end tests. They may still have minor bugs but are generally stable. Users in this stage can provide feedback before the API becomes GA. For example, the flow control group is currently in Beta.

After being in Beta for some time and passing multiple releases, the API becomes GA (stable). In GA, the version number no longer includes Alpha or Betaâ€”it is just v1. GA APIs are highly reliable, included in conformance tests, and supported in future Kubernetes releases. Most commonly used API groups, such as apps, authentication, authorization, certificates, and coordination, are now GA.

Some API groups support multiple versions simultaneously. For example, the autoscaling group may have v1, v2beta1, v2beta2, etc. This allows users to create objects using any supported version in their YAML files. For instance, an nginx.yaml can specify apiVersion: v1alpha1, v1beta1, or v1.

However, even with multiple versions, only one can be the preferred version and one can be the storage version. The preferred version is the default version used when retrieving information via kubectl get or kubectl explain. The storage version is the version in which objects are stored in etcd, regardless of which version was used in the YAML file. Preferred and storage versions are usually the same but can differ.

To identify the preferred version, you can list the API group using the API server URL. For example, querying the batch API group shows v1 and v1beta1 as supported versions, with v1 as the preferred version. Currently, there is no direct command to check the storage version; it can only be verified by querying the etcd database directly. For instance, querying a deployment object in etcd might show apiVersion: apps/v1.

To enable or disable specific API versions, especially Alpha APIs not enabled by default, you must add them to the --runtime-config parameter of the Kube API server service. Multiple APIs can be enabled by comma-separating them. After making changes, always restart the API server service for them to take effect. Once done, you can use and test those APIs.

That concludes this lecture on Kubernetes API versions.

# API Deprecations  
In this lecture, we will now discuss API deprecations in Kubernetes.

As we already know, a single API group can support multiple versions at a time. But this raises a few important questions: Why do we need to support multiple versions? How many versions should we support? And when is it safe to remove an older version that is no longer required? These questions are answered by the API deprecation policy. By looking at the answers, we also understand the rules that govern how APIs evolve over time.

* API Lifecycle Example
To understand this better, letâ€™s walk through the life cycle of an API group. Suppose we are contributing to the Kubernetes project and create an API group called codecloud.com with two resources: Course and Webinar.
After developing and testing in-house, we raise a PR to merge it into Kubernetes. If it gets accepted, we release it as an alpha version, called v1alpha1. At this stage, users can create resources using the API version codecloud.com/v1alpha1.
Now, letâ€™s say the Webinar resource does not work well with users, and we decide to remove it in the next release. Can we just delete it from v1alpha1? No. This brings us to the first rule of API deprecation policy:
ðŸ‘‰ API elements may only be removed by incrementing the version of the API group.
That means we can remove Webinar only in v1alpha2, while it must continue to exist in v1alpha1.

* Supporting Multiple Versions
At this point, we face a challenge. Our database still holds v1alpha1, but the API now has v1alpha2. To avoid breaking compatibility, Kubernetes must support both versions simultaneously. Internally, the preferred version could be v1alpha2, but users may continue using v1alpha1.
This leads to the second rule of API deprecation policy:
ðŸ‘‰ API objects must be able to round-trip between versions without losing information.
For example, if we create a Course object in v1alpha1, convert it to v1alpha2, and then back to v1alpha1, it should match the original definition. If a new field like duration was introduced in v1alpha2, we must provide an equivalent field or handling mechanism in v1alpha1 to maintain consistency.

* Evolution from Alpha â†’ Beta â†’ GA
Over time, the API progresses:
    Starts with v1alpha1, v1alpha2, and more alpha versions.
    Then moves to beta (v1beta1, v1beta2, etc.).
    Finally reaches stable GA (v1).
Of course, not all versions stay forever. Older ones are deprecated and eventually removed. But when exactly can this happen? Thatâ€™s where Rule 4A of the Kubernetes API deprecation policy comes in.

* Deprecation Policy Rules
Alpha versions â†’ Need not be supported once a new alpha is released.
Beta versions â†’ Must be supported for at least 9 months or 3 releases, whichever is longer.
GA versions â†’ Must be supported for at least 12 months or 3 releases, whichever is longer.
So, for example:
When v1alpha2 is released, v1alpha1 can be removed immediately.
When v1beta2 is released, v1beta1 must still exist for at least 3 more releases before being removed.
Similarly, a GA version (v1) cannot be deprecated until a new GA version (like v2) is released.
This ensures stability for production systems.

* Preferred and Storage Versions
Another rule (Rule 4B) states that:
ðŸ‘‰ The preferred API version and storage version may not advance until after a release supports both the new and old versions.
So when v1beta2 first appears alongside v1beta1, Kubernetes cannot yet switch its preferred version. Only in the next release, where both versions coexist, can v1beta2 become the preferred and storage version.

* Moving from v1 to v2
Now imagine Kubernetes introduces a v2alpha1 version. Can it deprecate v1 immediately?
No. According to Rule 3, an alpha version cannot deprecate a GA version. Only a new GA version can deprecate the old GA version. So v2alpha1 must progress through its lifecycle (alpha â†’ beta â†’ GA) before it can deprecate v1.

* Updating Manifests: kubectl convert
When old API versions are removed, users must update their manifest files to the new version. Doing this manually for many YAML files can be tedious.
Kubernetes provides the kubectl convert command for this. For example, if you have a deployment manifest using apps/v1beta1 and the cluster has removed that version, you can run:
kubectl convert -f old-deployment.yaml --output-version=apps/v1

This outputs the manifest in the new API version. Note that kubectl convert is not installed by defaultâ€”itâ€™s a plugin that must be added separately, with installation instructions available in the official Kubernetes documentation.

* Conclusion
In summary, the API deprecation policy ensures a smooth evolution of APIs in Kubernetes. It balances innovation with backward compatibility by defining clear rules around when versions can be deprecated or removed, how long they must be supported, and how users can migrate using tools like kubectl convert.

# Custom Resource Definition  
Instructor: Let's look at custom resource definitions in Kubernetes.

Before we talk about custom resources, let's first talk about resources. Hereâ€™s a deployment definition file. When we create a deployment, Kubernetes stores its information in the ETCD data store. We can create the deployment, list it, check its status, or delete it. All of this essentially manages the deployment object or resource in the ETCD data store.

However, when we create a deployment, it also creates pods equal to the number of replicas definedâ€”in this case, three. Who is responsible for that? Thatâ€™s the job of a controller, specifically the deployment controller. We donâ€™t need to manually create this controller because it is built into Kubernetes. A controller is a process that continuously runs in the background, monitoring resources it manages and making necessary changes to match the defined state. For example, creating a deployment object leads the controller to create a ReplicaSet, which in turn creates the defined number of pods.

Throughout this course, weâ€™ve seen many resources such as ReplicaSets, Deployments, Jobs, CronJobs, StatefulSets, and Namespaces. All of these resources have controllers that manage their state in the cluster. 

Now letâ€™s do something more interesting. Just like we created a deployment, suppose we want to create a custom objectâ€”say, a **FlightTicket** object to book flights. This could represent any real-world use case. For example, we define an object with API version `flights.com/v1`, kind `FlightTicket`, and name `my-flight-ticket`. In the spec, we define details like `from` (Mumbai), `to` (London), and `number` (2 tickets). When we create this object, we expect a flight ticket resource to be created. When we list or delete flight tickets, they should appear or be removed. 

But hereâ€™s the challenge: this only creates and manages data in ETCD. It doesnâ€™t actually book a ticket. To make this resource functional, we need a **controller**. Suppose there is an external API at `book-flight.com/api`. A custom controller can be written in Go to watch for FlightTicket objects. When a ticket is created, the controller will call the external API to book the ticket. When a ticket is deleted, the controller can call the API to cancel the booking. In this setup, the custom resource is the FlightTicket, and the custom controller makes it functional.

Now, if you try to create a FlightTicket resource today, it will fail. Kubernetes will complain that it doesnâ€™t recognize this kind. This is because we must first **define the resource type** in the Kubernetes API. To do this, we use a **CustomResourceDefinition (CRD)**. A CRD tells Kubernetes about new resource types we want to use.

A CRD has a similar structure to other objects, with API version, kind, metadata, and spec. The API version is `apiextensions.k8s.io/v1` and the kind is `CustomResourceDefinition`. The metadata contains the name, e.g., `flighttickets.flights.com`. In the spec, we define the scope (namespaced or cluster-wide), the API group (`flights.com`), the resource kind (`FlightTicket`), and the plural/singular names (`flighttickets` and `flightticket`). Optionally, we can define short names such as `ft` for convenience.  

Next, we configure versions. A resource may have multiple versions (alpha, beta, GA). In the CRD, we define which versions are served by the API and which is the storage version. Then, we define the schema for the resource using OpenAPI v3. For FlightTicket, the schema might include properties such as `from` (string), `to` (string), and `number` (integer). We can also add validations, such as minimum and maximum values for `number`.

Once the CRD is created with `kubectl create`, we can now create FlightTicket objects, list them, and delete them. This solves the first partâ€”Kubernetes now recognizes our new resource type. But remember, without a controller, these objects donâ€™t actually do anything. They are just stored in ETCD.

The second part is building a custom controller. A controller watches for these custom resources and performs actions when they are created, updated, or deleted. Without it, the resources are just static data. In the upcoming section, weâ€™ll see how to build such a controller for FlightTickets.

# Custom Controllers  
Instructor: In this lecture, we will look at developing Custom Controllers.

To pick up from where we left off, we have already created a Custom Resource Definition (CRD) and are able to create our FlightTicket objects. The data about these tickets is stored in ETCD. Now, what we need is a way to monitor the status of these objects in ETCD and perform actions such as making calls to the flight booking API to book, edit, or cancel flight tickets. This is exactly why we need a Custom Controller.

A controller is any process or code that runs in a continuous loop, constantly monitoring the Kubernetes cluster and listening to events when specific objects are created, modified, or deleted. In this case, the object is our FlightTicket resource. The controller reacts to these events and takes action accordingly.

Now, a controller can be developed in many ways. For example, if I know Python, I could write a Python program that queries the Kubernetes API server for FlightTicket objects, watches for changes, and then makes API calls to the external flight booking system. However, developing a controller in Python can be challenging. The API calls may become expensive, and we would need to build our own queuing and caching mechanisms from scratch.

Developing a controller in Go using the Kubernetes Go client is generally a better approach. It provides built-in support through libraries such as shared informers, which offer caching and queuing mechanisms. These libraries make it easier to build efficient and production-ready controllers in the right way.

So, how do you start building a Custom Controller? Kubernetes provides a GitHub repository named **sample-controller**. The process begins by cloning this repository. Inside, you will find a `controller.go` file, which you can modify with your own custom logic. After that, you build and run the controller. For this, you need to have the Go programming language installed on your system. If itâ€™s not already installed, youâ€™ll need to set it up before proceeding.

Once you have Go installed, clone the sample-controller repository and navigate into its directory. Next, you customize the `controller.go` file with your own logic. For example, somewhere inside the code, you might add a function to call the external flight booking API whenever a FlightTicket object is created. After customizing the code, build the project and run it, specifying the kubeconfig file so the controller can authenticate with the Kubernetes API server. At this point, the controller process starts running locally and begins watching for FlightTicket objects. Whenever new tickets are created, the controller makes the necessary API calls.

When your controller is ready, you can choose how to distribute it. Instead of manually building and running it each time, you might package it into a Docker image and run it inside your Kubernetes cluster as a Pod or Deployment. This way, it becomes part of the cluster itself and runs continuously like other controllers.

That is a high-level overview of building a Custom Controller. Now, in the Kubernetes exam, you are not expected to write a Custom Controller from scratch, since that involves coding knowledge. So it is unlikely that youâ€™ll get a direct question about controller implementation. However, you should definitely expect questions about creating and managing Custom Resource Definitions (CRDs), as well as working with existing controllers already available in the cluster.


# Operator Framework  
Narrator: Letâ€™s now look at the Operator Framework.

So far, we have talked about creating a Custom Resource Definition (CRD) and a Custom Controller that contains the logic to work with that CRD. At this point, these are separate entities. We must manually create the CRD and the resources defined by it, and then separately deploy the controller as a Pod or Deployment within Kubernetes.

However, with the Operator Framework, these two components can be packaged together and deployed as a single unit. For example, when we deploy a â€œFlight Operator,â€ it not only creates the Custom Resource Definition and associated resources, but also deploys the custom controller as a Kubernetes Deployment. In this way, both the CRD and controller are bundled into a single package, simplifying management.

The Operator Framework goes beyond just deploying these two components. Letâ€™s look at a real-life example. One of the most popular operators is the **etcd operator**. This operator is used to deploy and manage an etcd cluster within Kubernetes. It defines an etcd cluster CRD, along with a custom controller that watches for etcd cluster resources and ensures that etcd is properly deployed inside the cluster.

But it doesnâ€™t stop there. The etcd operator also provides advanced features such as taking backups of the etcd cluster and restoring those backups when needed. These tasks are accomplished through additional CRDs and supporting code, often referred to as backup and restore operators. In short, the operator encapsulates tasks that a human administrator would normally perform, automating them entirely.

This is the power of Kubernetes operators. They perform the duties of a human operatorâ€”installing the application, maintaining it, taking backups, restoring data in the event of disasters, and handling issues that arise during the lifecycle of the application.

Operators are available in the **OperatorHub**, which hosts a wide variety of operators for popular applications. Examples include etcd, MySQL, Prometheus, Grafana, ArgoCD, and Istio. You can browse available operators, read detailed documentation, and follow simple installation steps. Typically, deploying an operator involves three steps: first, installing the Operator Lifecycle Manager (OLM); second, installing the operator itself; and finally, creating the custom resources to trigger the desired functionality.

That is a high-level overview of operators. Diving deeper into the Operator Framework and operator development would require its own mini course, which we will cover at a later stage. For now, operators are not expected to appear in detail on the exam. The curriculum primarily emphasizes CRDs, so this lecture on operators should be treated as additional knowledge.

Thank you so much, and I will see you in the next lecture.

