# Readiness and Liveness Probes
In this section, we begin learning about observability in Kubernetes. Specifically, we’ll cover three important concepts: readiness probes, liveness probes, and logging and monitoring. Let’s start with readiness probes.
To understand readiness probes properly, let’s quickly recap the lifecycle of a Pod.
When a pod is first created, it enters the Pending state. At this stage, the scheduler is still deciding which node to place it on. If no node matches, the pod remains pending. Once scheduled, the pod status changes to ContainerCreating, during which Kubernetes pulls the required images and starts the containers. Once all containers are running, the pod reaches the Running state, where it will stay until it completes successfully or is terminated.
You can check this status anytime with:
kubectl get pods

And for more detailed information, including why a pod might be stuck, you can use:
kubectl describe pod <pod-name>

Now, besides the high-level pod status, Kubernetes also tracks conditions. Conditions are boolean values that provide finer details about the pod’s health. Examples include:
* PodScheduled → true once the pod is scheduled on a node.
* Initialized → true after initialization is complete.
* ContainersReady → true when all containers are marked ready.
* Ready → true when the pod as a whole is considered ready to serve traffic.
And it’s this Ready condition that we’re most interested in for readiness probes.

Here’s the problem: by default, Kubernetes assumes that a container is ready as soon as it starts. But in reality, many applications need extra time before they can actually serve traffic.
* A simple script may be ready in milliseconds.
* A database might take several seconds to initialize.
* A large web server such as Jenkins may take 10–15 seconds before its UI is even accessible.
During this warm-up phase, Kubernetes still reports the pod as ready, and the service immediately starts sending user traffic to it. This can cause failed requests, service disruption, and frustrated users.

So how do we fix this? We need a way to tie the pod’s ready condition to the real readiness of the application inside. That’s exactly what readiness probes do.
As the developer of the application, you know best when your app is truly ready. Kubernetes allows you to define a probe to test this. The probe can be configured in three main ways:
* HTTP GET → check if a given endpoint responds. Perfect for web apps and APIs.
* TCP Socket → test whether a particular port is open. Useful for databases and services that expose ports.
* Exec Command → run a command inside the container and consider it ready only if the command exits successfully.

Here’s an example of configuring an HTTP readiness probe in a pod spec:
    readinessProbe:
    httpGet:
        path: /ready
        port: 8080
    initialDelaySeconds: 10
    periodSeconds: 5
    failureThreshold: 3

This tells Kubernetes: wait 10 seconds before checking, then probe every 5 seconds, and only mark the pod ready if the endpoint responds successfully. If it fails three times in a row, mark it as not ready.
You can customize these options further:
* initialDelaySeconds → how long to wait before starting checks.
* periodSeconds → how often to run the probe.
* failureThreshold → how many failures before the pod is marked unready.

Now let’s see why readiness probes are especially important in multi-pod deployments. Imagine you have a deployment with two pods already serving traffic. You scale up and add a third pod. If readiness probes are not configured, Kubernetes will immediately start routing traffic to the new pod—even though it might take a full minute to warm up. That means some users will hit a pod that’s not ready, causing errors.
But if you define readiness probes correctly, the service will continue sending requests only to the healthy pods. Once the new pod passes the readiness check, it’s marked as ready, and traffic is routed to it seamlessly. No downtime, no user disruption.
And that’s the power of readiness probes—they ensure Kubernetes only routes traffic to containers that are genuinely ready to serve users.
That’s it for this lecture. Head over to the coding exercises and practice configuring readiness probes in your own pod definitions.

# Liveness Probes
In the last lecture, we looked at readiness probes. Now, let’s talk about the second important type of probe in Kubernetes: the liveness probe.
Let’s start with a simple example.
Imagine you’re running an NGINX container with Docker. Everything works fine at first—it serves traffic normally. But then, for some reason, the NGINX process crashes and exits. The container itself stops running. Since Docker by itself is not an orchestrator, the container simply stays down until you intervene manually and start a new one.
Now, let’s move this same application into Kubernetes.
Here’s where Kubernetes orchestration helps us. If the application crashes and the container exits, Kubernetes automatically detects this and restarts the container for you. You can even see the restart count increase when you run:
* kubectl get pods

So far so good. But here’s the tricky scenario:
What if the container process is technically running, but the application itself is stuck?
For example, due to a bug in the code, your web server might enter an infinite loop. The container doesn’t crash—it stays alive. From Kubernetes’ perspective, the container is “healthy,” but from a user’s perspective, the application isn’t responding at all.
This is where liveness probes become crucial.
A liveness probe allows you to define a health check that Kubernetes will run periodically on the application inside the container. If the probe fails, Kubernetes considers the container unhealthy. It will then kill the container and create a fresh one.

As the application developer, you decide what “healthy” means for your workload.
* For a web application, that might mean checking if the /healthz or /live API endpoint responds with a 200 OK.
* For a database, you might check whether a specific TCP port is accepting connections.
* Or you could simply run a command inside the container that exits successfully if the app is healthy.

Here’s a quick example of how a liveness probe looks in a pod spec:
    livenessProbe:
    httpGet:
        path: /healthz
        port: 8080
    initialDelaySeconds: 10
    periodSeconds: 5
    failureThreshold: 3

This means: wait 10 seconds before starting checks, then probe every 5 seconds. If the check fails three times in a row, Kubernetes restarts the container.
Just like readiness probes, you can configure liveness probes in three ways:
* httpGet → perform an HTTP request.
* tcpSocket → check if a TCP port is open.
* exec → run a custom command inside the container.

And you have additional tuning options such as:
* initialDelaySeconds → when to start probing after container start.
* periodSeconds → how often to probe.
* successThreshold and failureThreshold → how many successes or failures are needed before changing state.

In short:
* Readiness probes decide whether a pod should receive traffic.
* Liveness probes decide whether a pod should be restarted.

Both are equally important for ensuring your applications stay reliable and self-heal automatically.
That’s it for this lecture. Head over to the coding exercises and try configuring liveness probes yourself. You’ll also get practice troubleshooting broken probes and fixing them.

# Logging
In this lecture, we’ll talk about logging mechanisms in Kubernetes.
Let’s begin with something familiar: logging in Docker.
Suppose I run a container called event-simulator. This container simply generates random events—like a web server producing access logs. These events are written to the standard output of the application.
If I run this container in the foreground, I’ll see the logs directly on my terminal. But if I run it in detached mode using the -d flag, I won’t see any logs immediately. To view them, I can use:
* docker logs <container-id>

And if I want to keep watching the log stream live, I can add the -f option, just like tail -f.
Now let’s switch back to Kubernetes.
If I create a Pod that runs the same event-simulator image, once the Pod is up and running, I can view its logs with:
* kubectl logs <pod-name>

And just like with Docker, I can use -f to stream logs in real time.
But here’s something important to remember: Kubernetes Pods can run multiple containers.
For example, let’s say I update my Pod definition to include a second container called image-processor.
Now, if I just run:
* kubectl logs <pod-name>

Kubernetes won’t know which container’s logs to display. In this case, the command will fail and prompt me to specify the container explicitly.
So I must run:
* kubectl logs <pod-name> -c event-simulator
or
* kubectl logs <pod-name> -c image-processor

depending on which container’s logs I want to view.
And that’s the core logging mechanism built into Kubernetes.
It’s simple, lightweight, and usually enough for application developers to get started. For the CKAD exam, this level of knowledge is all you need—viewing logs from Pods and containers using kubectl logs.
That’s it for this lecture.
Head over to the coding exercises and try it yourself—create a multi-container Pod and practice viewing logs from each container individually.
Thank you, and I’ll see you in the next lecture.

# Monitor and debug application
In this lecture, we’ll talk about monitoring a Kubernetes cluster
So, how do you monitor resource consumption in Kubernetes? And what exactly would you like to monitor?
At a node level, you might want to know:
    How many nodes are in the cluster.
    How many nodes are healthy.
    Resource usage, such as CPU, memory, network, and disk utilization.
At a pod level, you might want to know:
    How many pods are running.
    CPU and memory consumption per pod.
    Essentially, we need a solution that collects metrics, stores them, and provides analytics.

As of this recording, Kubernetes does not come with a full-featured built-in monitoring solution. However, there are a number of open-source solutions available, such as:
    Metrics Server
    Prometheus
    The Elastic Stack (ELK/EFK)
And there are also proprietary solutions like Datadog and Dynatrace.

For this course and the CKAD certification, you only need basic knowledge of monitoring Kubernetes, which is covered by the Metrics Server. The other solutions are part of the Kubernetes for Administrators course.
Originally, the project called Heapster enabled monitoring and analysis for Kubernetes. You might see references to Heapster in older tutorials. However, Heapster is now deprecated, and its lightweight successor is the Metrics Server.
The Metrics Server works like this:
    You can have one Metrics Server per cluster.
    It retrieves metrics from all nodes and pods in the cluster.
    Metrics are aggregated and stored in memory.
Important note:
    Metrics Server is in-memory only.
    Historical metrics are not stored on disk.
    For historical data, you would need Prometheus or another advanced monitoring solution.

Now, how does Metrics Server get its data?
Every Kubernetes node runs a Kubelet, which is responsible for:
    Communicating with the Kubernetes API server.
    Running pods on the node.
The Kubelet includes a component called cAdvisor (Container Advisor).
    cAdvisor collects performance metrics from each pod.
    It exposes these metrics via the Kubelet API for the Metrics Server.

If you’re using Minikube, you can enable the Metrics Server with:
* minikube addons enable metrics-server

For other environments, you deploy the Metrics Server manually:
    Clone the Metrics Server deployment files from GitHub.
    Use kubectl create -f <deployment-files> to deploy the necessary pods, services, and roles.

After deployment, give the Metrics Server some time to collect metrics.
Once ready, you can check node-level performance metrics with:
* kubectl top nodes

This will show CPU and memory usage for each node.
For example, the master node might show 8% CPU consumed, which is roughly 166 millicores.
To view pod-level metrics, run:
* kubectl top pods
This provides CPU and memory usage for each pod in the cluster.
And that’s it for this lecture!
Head over to the coding exercises and practice viewing metrics in your Kubernetes cluster.
Thank you.

