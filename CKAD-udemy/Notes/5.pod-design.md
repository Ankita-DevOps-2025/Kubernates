# Labels , selector and annotation
In this lecture, we will talk about labels, selectors, and annotations in Kubernetes.
What are labels and selectors?
Labels and selectors are a standard method to group and filter objects.
Think of it this way:
    You have a set of animals.
    You want to filter them based on criteria like class, kind, color, or whether they are domestic or wild.
    You may want a single filter (e.g., all green animals) or multiple filters (e.g., all green birds).
The best way to handle this is with labels:
    Labels are key-value properties attached to each item.
    Selectors allow you to filter objects based on these labels.
Example:
    class=mammal → lists all mammals
    color=green → lists all green mammals
We see labels and selectors in real life:
    Keywords on YouTube videos or blogs for filtering.
    Product filters in online stores.
How are labels and selectors used in Kubernetes?
Kubernetes has many types of objects: pods, services, replica sets, deployments, etc.
Over time, you may have hundreds or thousands of objects, so you need a way to filter and group them by:
    Type
    Application
    Functionality
Solution: Use labels and selectors.
Example:
1. In a pod definition file, under metadata, create a section called labels.
2. Add labels in key-value format:
metadata:
  labels:
    app: app1
    environment: dev

3. Create the pod.
4. Use a selector to filter pods:
kubectl get pods --selector app=app1

* Labels and selectors in Kubernetes objects
    Labels and selectors are used internally to connect different objects.
* Example: ReplicaSet
    You create a ReplicaSet for 3 pods.
    Step 1: Label the pods.
    Step 2: In the ReplicaSet, use selector to match the pod labels.
Important: Labels appear in two places in a ReplicaSet definition:
1. Under template → these are the pod labels
2. At the top → these are the ReplicaSet’s own labels

For the ReplicaSet to discover pods, only the pod labels matter.
Selector rules:
    A single label is enough if unique.
    Multiple labels can ensure correct pods are selected if some labels might overlap.
This works the same for a Service:
    The service uses selector to match labels on pods (or ReplicaSets) and route traffic accordingly.

What about annotations?
While labels and selectors are used for grouping and selection, annotations are used for informational purposes:
    Example: Tool details like name, version, build info
    Contact information like email or phone
    Integration metadata
Annotations do not affect scheduling or selection. They are purely for metadata purposes.
That’s it for this lecture on labels, selectors, and annotations.

# Rolling Updates & Rollbacks in Deployments
Today, we will talk about updates and rollbacks in a deployment.

1. Rollouts and Versioning
    When you create a deployment, Kubernetes triggers a rollout.
    Each rollout creates a new deployment revision (e.g., Revision 1).
    Updating the container version triggers a new rollout and creates a new revision (e.g., Revision 2).
    Revisions help track changes and allow rollback to a previous version if needed.
Commands to check rollout status and history:
    kubectl rollout status deployment <deployment-name>
    kubectl rollout history deployment <deployment-name>

2. Deployment Strategies
There are two main deployment strategies:
Recreate Strategy
    All old pods are terminated first.
    New pods are then created.
    Problem: downtime occurs between old pods stopping and new pods starting.
    Events: old ReplicaSet scaled to 0, new ReplicaSet scaled up.
Rolling Update Strategy (default)
    Pods are updated one by one.
    Old pods are gradually replaced with new pods.
    No downtime; upgrade is seamless.
    Events: old ReplicaSet scaled down gradually while new ReplicaSet scaled up gradually.

3. Updating a Deployment
You can update a deployment in multiple ways:
1. Modify the deployment YAML
    Change fields like image version, labels, or replica count.
    Apply changes using:
        kubectl apply -f deployment.yaml
2. Use kubectl set image
    kubectl set image deployment/<deployment-name> <container-name>=<new-image>

⚠️ Caution: Using set image may differ from your YAML file, so future updates via YAML need care.

4. What Happens Under the Hood
Deployment automatically creates a ReplicaSet, which manages the pods.
During an update:
    A new ReplicaSet is created for the updated version.
    Pods from the old ReplicaSet are gradually replaced (rolling update).
You can list ReplicaSets:
    kubectl get rs
Old ReplicaSet may show 0 pods, new one shows the updated pod count.

5. Rollbacks
If something goes wrong with the new version, you can roll back:
    kubectl rollout undo deployment <deployment-name>
Old ReplicaSet is restored; new ReplicaSet is removed.
Pods are switched back to the previous version seamlessly.

6. Quick Summary of Commands
Action	                                 Command
Create deployment	                    kubectl create -f deployment.yaml
List deployments	                    kubectl get deployments
Update deployment (YAML)             	kubectl apply -f deployment.yaml
Update image only	                    kubectl set image deployment/<name> <container>=<image>
Check rollout status	                kubectl rollout status deployment/<name>
Rollback deployment                  	kubectl rollout undo deployment/<name>

This covers how deployments manage updates, rollouts, and rollbacks.

# Updating a Deployment
Here are some handy examples related to updating a Kubernetes Deployment:
Creating a deployment, checking the rollout status and history:
In the example below, we will first create a simple deployment and inspect the rollout status and the rollout history:
    master $ kubectl create deployment nginx --image=nginx:1.16
    deployment.apps/nginx created
    
    master $ kubectl rollout status deployment nginx
    Waiting for deployment "nginx" rollout to finish: 0 of 1 updated replicas are available...
    deployment "nginx" successfully rolled out
    
    master $
 
    master $ kubectl rollout history deployment nginx
    deployment.extensions/nginx
    REVISION CHANGE-CAUSE
    1     <none>
    
    master $


Using the --revision flag:
Here the revision 1 is the first version where the deployment was created.
You can check the status of each revision individually by using the --revision flag:
    master $ kubectl rollout history deployment nginx --revision=1
    deployment.extensions/nginx with revision #1
    
    Pod Template:
    Labels:    app=nginx    pod-template-hash=6454457cdb
    Containers:  nginx:  Image:   nginx:1.16
    Port:    <none>
    Host Port: <none>
    Environment:    <none>
    Mounts:   <none>
    Volumes:   <none>
    master $ 

Using the --record flag:
You would have noticed that the "change-cause" field is empty in the rollout history output. We can use the --record flag to save the command used to create/update a deployment against the revision number.
    master $ kubectl set image deployment nginx nginx=nginx:1.17 --record
    deployment.extensions/nginx image updated
    master $master $
    
    master $ kubectl rollout history deployment nginx
    deployment.extensions/nginx
    
    REVISION CHANGE-CAUSE
    1     <none>
    2     kubectl set image deployment nginx nginx=nginx:1.17 --record=true
    master $

You can now see that the change-cause is recorded for the revision 2 of this deployment.
Let's make some more changes. In the example below, we are editing the deployment and changing the image from nginx:1.17 to nginx:latest while making use of the --record flag.
    master $ kubectl edit deployments. nginx --record
    deployment.extensions/nginx edited
    
    master $ kubectl rollout history deployment nginx
    REVISION CHANGE-CAUSE
    1     <none>
    2     kubectl set image deployment nginx nginx=nginx:1.17 --record=true
    3     kubectl edit deployments. nginx --record=true
    
    master $ kubectl rollout history deployment nginx --revision=3
    deployment.extensions/nginx with revision #3
    
    Pod Template: Labels:    app=nginx
        pod-template-hash=df6487dc Annotations: kubernetes.io/change-cause: kubectl edit deployments. nginx --record=true
    
    Containers:
    nginx:
    Image:   nginx:latest
    Port:    <none>
    Host Port: <none>
    Environment:    <none>
    Mounts:   <none>
    Volumes:   <none>
    
    master $

Undo a change:
Lets now rollback to the previous revision:
    controlplane $ kubectl rollout history deployment nginx
    deployment.apps/nginx 
    REVISION  CHANGE-CAUSE
    1         <none>
    3         kubectl edit deployments.apps nginx --record=true
    4         kubectl set image deployment nginx nginx=nginx:1.17 --record=true
    
    controlplane $ kubectl rollout history deployment nginx --revision=3
    deployment.apps/nginx with revision #3
    Pod Template:
    Labels:       app=nginx
            pod-template-hash=787f54657b
    Annotations:  kubernetes.io/change-cause: kubectl edit deployments.apps nginx --record=true
    Containers:
    nginx:
        Image:      nginx:latest
        Port:      <none> 
        Host Port:  <none>
        Environment: <none>       
        Mounts:     <none>
    Volumes:      
    
    controlplane $ kubectl describe deployments. nginx | grep -i image:
        Image:        nginx:1.17
    
    controlplane $


With this, we have rolled back to the previous version of the deployment with the image = nginx:1.17.
    controlplane $ kubectl rollout history deployment nginx --revision=1
    deployment.apps/nginx with revision #1
    Pod Template:
    Labels:       app=nginx
            pod-template-hash=78449c65d4
    Containers:
    nginx:
        Image:      nginx:1.16
        Port:       <none> 
        Host Port:  <none>
        Environment: <none>     
        Mounts:     <none>
    Volumes:      
 
controlplane $ kubectl rollout undo deployment nginx --to-revision=1
deployment.apps/nginx rolled back

To rollback to specific revision we will use the --to-revision flag.
With --to-revision=1, it will be rolled back with the first image we used to create a deployment as we can see in the rollout history output.
    controlplane $ kubectl describe deployments. nginx | grep -i image:
    Image: nginx:1.16

# Demo Deployment
 Demo: Updates and Rollbacks in a Deployment
1. Check Existing Resources
kubectl get all
Confirms there are no existing deployments or pods except default services.

2. Create a Deployment
kubectl create deployment my-app --image=nginx --replicas=6 --record
--record records the command in rollout history.
Check rollout status:
kubectl rollout status deployment my-app
Pods are deployed one at a time (rolling update style).
Check rollout history:
kubectl rollout history deployment my-app
Revision 1 is created with the creation command recorded.

3. Update Deployment
Change the image in the deployment YAML or via kubectl apply.
Change image in deployment.yaml to nginx:1.12
kubectl apply -f deployment.yaml
kubectl rollout status deployment my-app
kubectl rollout history deployment my-app
Creates Revision 2 for the updated image
Alternative update using set image:
kubectl set image deployment/my-app nginx=nginx:1.12-pull
kubectl rollout status deployment my-app
kubectl rollout history deployment my-app
Creates Revision 3.

4. Rollback Deployment
If the latest change causes issues:
kubectl rollout undo deployment my-app
kubectl rollout status deployment my-app
kubectl rollout history deployment my-app
Kubernetes creates a new revision (e.g., Revision 4) pointing back to the stable version (Revision 2).
Pods revert to the previous working image.

5. Simulate an Error
Update the deployment to a non-existent image (e.g., nginx:1.5-ERL) and apply:
kubectl apply -f deployment.yaml --record
kubectl rollout status deployment my-app

Rollout gets stuck because the new image cannot be pulled.
Kubernetes pauses the rollout to avoid impacting users.
Check pod and deployment status:
kubectl get pods
kubectl get deployment
kubectl rollout history deployment my-app

6. Undo a Failed Deployment
kubectl rollout undo deployment my-app
kubectl rollout status deployment my-app
kubectl rollout history deployment my-app
Rollback creates a new revision (e.g., Revision 6) and restores the last stable image.
All pods are now running the previous working image.

✅ Key Points
--record stores the applied command in rollout history.
kubectl apply or kubectl set image triggers a new rollout.
Rolling updates deploy pods one at a time to avoid downtime.
kubectl rollout undo safely reverts to a previous revision.
Kubernetes pauses rollouts automatically if new pods fail to deploy.

# Deployment Strategy - Blue Green
In this lecture, we will now discuss additional deployment strategies. Earlier, we had talked about two basic strategies. The first is the recreate strategy, where a newer version is deployed by first destroying all of the existing versions and then creating newer versions of the application instances. The main problem with this approach is that during the time when the older versions are down and before the newer versions are up, the application is completely inaccessible to users.

The second is the rolling update strategy, where we do not destroy all instances at once. Instead, we take down the older version and bring up a newer version one by one. This way the application never goes fully down and the upgrade is seamless. Rolling update is also the default deployment strategy in Kubernetes.

Now, apart from these, there are a couple of other deployment strategies. These are not options that you can directly specify in the deployment manifest, but they can still be implemented using Kubernetes primitives. These are the blue-green deployment and canary updates.

The blue-green deployment strategy works by deploying the new version of the application alongside the old version. The old version is referred to as blue and the new version as green. Initially, 100% of the traffic is still routed to the old version. At this point, tests can be run on the green version without impacting users. Once all tests pass successfully, the traffic is switched from the blue version to the green version all at once. This makes the green version the new active version.

These advanced strategies are often best implemented with service meshes like Istio, which provide powerful traffic management features. In this lecture, however, we will look at how to implement them using just native Kubernetes deployments and services.

Here’s how the process works. First, we have the original version of our application deployed as a service. This is our blue deployment. We create a service to route traffic to it, and to associate the service with the pods, we set a label on the pods, for example version=v1. The service uses the same label as its selector. Next, we deploy a second deployment with the newer version of the application, called green. This time, we use the label version=v2.

At this stage, no traffic is being routed to the green version, so we are free to test it and ensure it works as expected. Once everything is validated, all we need to do is update the service selector from version=v1 to version=v2. The service then routes all traffic to the green deployment, completing the blue-green switch.

That is how the blue-green deployment strategy can be implemented in Kubernetes using deployments and services.


# Deployment Strategy - Canary
In this lecture, we will now look at canary updates in Kubernetes. This strategy allows us to deploy the new version of the application while routing only a small percentage of traffic to it. The majority of traffic continues to flow to the older version, but a small portion is directed to the new version. At this stage, we can run tests, monitor the performance, and if everything looks good, we can then upgrade the original deployment with the newer version. Once the upgrade is complete, the canary deployment can be removed.

Let’s now see how we can implement this strategy using native Kubernetes primitives like deployments and services. First, we start with the original version of the application deployed as a deployment, which we will call the primary deployment. For example, it could have five pods running. Along with this, we create a service to route traffic to the pods in this deployment. To associate the service with the pods, we use labels. For example, we might label the pods with version=v1, and the service selector is set to match this label.

Next, we create a second deployment that represents our canary version of the application. This will be the new version of the application image. Initially, all traffic is still going to version 1. With a canary deployment, however, we want traffic to be routed to both versions at the same time, but only a small portion should go to version 2.

The first step is to make sure the service can send traffic to both deployments. To do this, we introduce a common label, for example app=frontend. We then update the service selector to match this label. This way, the service can now route traffic to both the primary and canary deployments. At this point, however, the traffic is distributed equally across both deployments, meaning 50% goes to each.

To reduce the traffic going to the canary version, we simply reduce the number of pods in the canary deployment. For instance, if the primary deployment has five pods and the canary deployment has just one, then the service will route traffic equally among all six pods. This means that approximately 83% of traffic goes to the primary deployment and only about 17% goes to the canary deployment. In this way, we achieve a partial rollout.

Once tests are completed and we are confident that the new version is stable, we can proceed to upgrade the pods in the primary deployment with the new version. After this upgrade is successful, the canary deployment can be deleted, since all pods in the primary deployment are now running the new version.

One important limitation of implementing canary deployments this way in Kubernetes is that the traffic split depends on the number of pods in each deployment. For example, we cannot precisely route just 1% of traffic to the canary version unless we have at least 100 pods in total. This makes fine-grained control difficult.

For more accurate control, service meshes like Istio provide powerful traffic management features. With Istio, you can define the exact percentage of traffic to send to each version, independent of the number of pods. For example, even if you have only one pod in each deployment, you can configure Istio to send 1% of traffic to the canary version and 99% to the primary version.

Finally, let’s look at how this can be implemented in code. First, you create the primary deployment, for example MyApp-Primary, and expose it using a service with a selector app=frontend. Next, you create the canary deployment, for example MyApp-Canary, which uses the newer image, say version 2.0. To limit the traffic, you configure only one replica for the canary deployment. Both deployments share the same label app=frontend, which ensures the service sends traffic to both sets of pods. With this setup, a small percentage of traffic automatically flows to the canary deployment.

That concludes this lecture on canary updates in Kubernetes. Now, head over to the practice labs to try this out hands-on and see it in action.

# Jobs
In this lecture, we will look at jobs in Kubernetes. Containers can serve different types of workloads. Some workloads, such as web servers, applications, and databases, are designed to run for a long period of time until they are manually taken down. These are continuous workloads meant to stay alive indefinitely.

On the other hand, there are workloads such as batch processing, analytics, or reporting, that are designed to carry out a specific task and then finish. Examples include performing a computation, processing an image, running analytics on a dataset, or generating and emailing a report. These workloads only need to live for a short time, perform their work, and then exit.

To better understand this, let’s look at how such a workload runs in Docker. Suppose we run a container to perform a math operation such as adding two numbers. The container starts, performs the task, prints the output, and exits. When you run docker ps, you see the container in an exited state with a return code. Since the task was completed successfully, the return code is zero.

We can replicate this behavior in Kubernetes by creating a Pod definition file that performs the same operation. When the Pod is created, it runs the container, completes the computation, and exits. However, the Pod is then recreated automatically, because Kubernetes expects applications to run continuously. This happens because the Pod’s restart policy defaults to Always, meaning Kubernetes tries to restart the container whenever it exits. To change this behavior, we can set the restart policy to Never or OnFailure. That way, Kubernetes won’t restart the container once the job is finished.

Now, this works fine for small tasks, but batch processing often requires processing large datasets with multiple Pods working in parallel. In such cases, we want a manager that can create several Pods, assign them tasks, and ensure completion. While ReplicaSets are used to keep a fixed number of Pods running at all times, Jobs are designed to run a set of Pods to perform a task to completion.

We create a job using a definition file. The API version is batch/v1, and the kind is Job. Under the spec, we define a template, which contains the Pod specification. Once the file is ready, we create the job using kubectl create. We can then check the status with kubectl get jobs, which shows if the job has completed successfully. Running kubectl get pods shows the Pods created by the job, typically in a Completed state with zero restarts, since they are not restarted after finishing.

To view the output of the job, we use the kubectl logs command on the Pod. This displays whatever the container wrote to standard output. Once finished, we can delete the job with kubectl delete job. Deleting the job also removes the Pods it created.

Of course, this is a simplified example. In real scenarios, jobs may process images and write results to persistent volumes, or generate reports and send them via email. But the concept remains the same: a job runs to completion and produces output.

By default, a job creates a single Pod. To run multiple Pods, we can specify the completions field in the job specification. For example, setting completions: 3 ensures that three Pods are run and complete successfully. Kubernetes runs these Pods one after another unless we specify otherwise.

But what happens if some Pods fail? Suppose we create a job using an image that randomly succeeds or fails. Kubernetes will continue creating new Pods until the desired number of successful completions is achieved. For example, if we set completions: 3, Kubernetes will keep retrying until three Pods finish successfully, regardless of how many failures occur.

We can also configure jobs to run Pods in parallel by using the parallelism property. For example, setting parallelism: 3 means Kubernetes will launch three Pods at once. If two succeed, Kubernetes will only create one more Pod to meet the total completion requirement.

That’s it for this lecture on Jobs in Kubernetes. Now, head over to the coding quiz, practice creating jobs, and experiment with completions and parallelism.

# CronJobs
In this lecture, we will look at CronJobs in Kubernetes.

A CronJob is a type of Kubernetes Job that can be scheduled, similar to using Crontab in Linux. For example, suppose you have a job that generates a report and sends it via email. If you create a regular Kubernetes Job, it runs instantly and then completes. But instead of running it manually each time, you can create a CronJob to schedule and run it periodically at specific times.

To create a CronJob, we begin with a blank template. The API version (at the time of this lecture) is batch/v1beta1, and the kind is CronJob (with a capital C and J). We can give it a name, such as Reporting-CronJob. Under the spec section, we must specify a schedule. The schedule uses a Cron-like format string, where you define the exact time and frequency at which the job should run.

The next important part is the job template, which contains the definition of the actual job that should be run. For this, we move the content from the spec section of a regular Job definition into the CronJob’s job template. Because of this nesting, the CronJob definition becomes more complex, as it now contains three separate spec sections—one for the CronJob itself, one for the Job, and one for the Pod.

Once the definition file is ready, we can create the CronJob using the kubectl create command. To verify, we run the kubectl get cronjob command, which shows the newly created CronJob. Kubernetes will then automatically create the required Jobs and Pods at the scheduled times.

That’s it for this lecture on CronJobs. Now, head over to the coding exercises and practice creating CronJobs to see them in action. I will see you in the next lecture.

