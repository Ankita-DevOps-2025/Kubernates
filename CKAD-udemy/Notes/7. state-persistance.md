# Introduction to Docker Storage  
# Storage in Docker  
# Volume Driver Plugins in Docker  
# Volumes  
# Persistent Volumes  
# Persistent Volume Claims  
# Using PVCs in Pods  
# Note on optional topics  
# Storage Classes  
# Why Stateful Sets?  
# Stateful Sets Introduction  
# Headless Services  
# Storage in StatefulSets  
========================================================================================================================================


# Introduction to Docker Storage  
Let us now look at storage in Kubernetes. To understand storage in container orchestration tools like Kubernetes, it is important to first understand how storage works with containers.

Understanding how storage works with Docker first and getting all the basics right will later make it so much easier to understand how it works in Kubernetes.

When it comes to storage in Docker, there are two concepts you must know about: storage drivers and volume driver plugins.

In the upcoming video, we will discuss about storage drivers. It's something that we've discussed in the course. So if you have gone through that already, feel free to skip this video or you may choose to stay and refresh your memory.

Once done, we will talk about volume drivers.

# Storage in Docker  
Hello and welcome to this lecture, and we are learning advanced concepts in this lecture. We are going to talk about Docker's storage drivers and filesystems. We're going to see where and how Docker stores data and how it manages filesystems after containers.

Let us start with how Docker stores data on the local filesystem. When you install it on a system, it creates this folder structure. Under /var/lib/docker, you have multiple folders under it called containers, images, volumes, etc. This is where Docker stores all its data by default. When I say data, I mean files related to images and containers running on the host. For example, all files related to containers are stored under the containers folder and the files related to images are stored under the images folder. Any volumes created by the Docker containers are created under the volumes folder. Well, don't worry about that for now. We will come back to that in a bit. For now, let's just understand where Docker stores its files and in what format.

So how exactly does Docker store the files of an image and a container? To understand that, we need to understand Docker's layered architecture. Let's quickly recap something we learned when Docker builds images. It builds these in a layered architecture. Each line of instruction in the Dockerfile creates a new layer in the Docker image with just the changes from the previous layer.

For example, the first layer is a base Ubuntu operating system, followed by the second instruction that creates a second layer which installs all the apt packages. And then the third instruction creates a third layer, which adds the Python packages, followed by the fourth layer that copies the source code over. And then finally, the fifth layer that updates the entry point of the image. Since each layer only stores the changes from the previous layer, it is reflected in the size as well. If you look at the base of the image, it is around 120 megabytes in size. The packages that are installed are around 300 MB, and then the remaining layers are small.

To understand the advantages of this layered architecture, let's consider a second application. This application has a different Dockerfile, but it's very similar to our first application as it uses the same basic image as Ubuntu, uses the same Python and Flask dependencies, but uses a different source code to create a different application, and a different entry point as well. When I run the Docker build command to build a new image for this application, since the first three layers of both the applications are the same, Docker is not going to build the first three layers. Instead, it uses the same three layers it built for the first application from the cache and only creates the last two layers with the new sources and the new entry point. This way, Docker builds images faster and efficiently, saving disk space.

This is also applicable if you were to update your application code. Whenever you update your application code, such as the app app.py, Docker simply reuses all the previous layers from cache and quickly rebuilds the application image by updating the latest source code, thus saving us a lot of time during rebuilds and updates. Let's rearrange the layers bottom up so we can understand it better. At the bottom, we have the base layer, then the packages, then the dependencies, and then the source code of the application, and then the entry point.

All of these layers are created when we run the Docker build command to form the final Docker image. So all of these are the Docker image layers. Once the build is complete, you cannot modify the contents of these layers. And so they are read-only and you can only modify them by initiating a new build.

When you run a container based off of this image using the docker run command, Docker creates a container based off of these layers and creates a new writable layer on top of the image layer. The writable layer is used to store data created by the container, such as log files, by the applications, any temporary files generated by the container, or just any file modified by the user on that container.

The life of this layer, though, is only as long as the container is alive. When the container is destroyed, this layer and all of the changes stored in it are also destroyed. Remember that the same image layer is shared by all containers created using this image. If I were to log into the newly created container and create a new file called 10.txt, it would create that file in the container layer, which is read-write.

We just said that the files in the image layer are read-only, meaning you cannot edit anything in those layers. Let's take an example of our application code. Since we bake our code into the image, the code is part of the image layer and as such is read-only after running a container. What if I wish to modify the source code, say test.py? Remember, the same image layer may be shared between multiple containers created from this image.

So does it mean that I cannot modify this file inside the container? Now I can still modify this file, but before I save the modified file, Docker automatically creates a copy of the file in the read-write layer, and I will then be modifying a different version of the file in the read-write layer. All future modifications will be done on this copy of the file in the read-write layer. This is called copy-on-write mechanism. The image layer being read-only just means that the files in these layers will not be modified in the image itself. So the image will remain the same all the time until you rebuild the image using the docker build command.

What happens when we get rid of the container? All of the data that was stored in the container layer also gets deleted. The changes we made to app.py and the new temp file we created will also get removed. So what if we wish to persist this data? For example, if we were working with a database and we would like to preserve the data created by the container, we could add a persistent volume to the container.

To do this, first, create a volume using the docker volume create command. So when I run the docker volume create data_volume command, it creates a folder called data_volume under the /var/lib/docker/volumes directory. Then when I run the container using the docker run command, I could mount this volume inside the Docker container’s writable layer using the -v option like this.

So I would do docker run -v then specify my newly created volume name, followed by a colon and the location inside my container, which is the default location where MySQL stores data. And that is where I mount my volume, and then the image name, mysql. This will create a new container and mount the data volume we created into our /var/lib/mysql folder inside the container, so all data written by the database is in fact stored on the volume created on the Docker host. Even if the container is destroyed, the data is still active.

Now, what if you didn't run the docker volume create command to create the volume before the docker run command? For example, if I run the docker run command to create a new instance of my mysql container with the volume data_volume2, which I have not created yet, Docker will automatically create a volume named data_volume2 and mount it to the container. You should be able to see all these volumes if you list the contents of the /var/lib/docker/volumes folder.

This is called volume mounting, as we are mounting a volume created by Docker under the /var/lib/docker/volumes folder. But what if we had our data already at another location? For example, let's say we have some external storage on the Docker host at /data and we would like to store database data on that volume and not in the default /var/lib/docker/volumes folder. In that case, we would run a container using the docker run -v command, but in this case we will provide the complete path to the folder we would like to mount. That is what was /large_data for MySQL, and so it will create a container and mount the folder to the container.

This is called bind mounting. So there are two types of mounts, a volume mount and a bind mount. Volume Mount mounts a volume from the volumes directory and bind mount mounts directly from any location on the Docker host.

One final point to note before I let you go, using the -v is an old style. The new way is to use the --mount option. The --mount is the preferred way as it is more verbose. So you have to specify each parameter in a key=value format. For example, the previous command can be written with the --mount option as this using the type, source, and target options. The type in this case is bind. The source is the location on my host and target is the location on my container.

So who is responsible for doing all of these operations, maintaining the layered architecture, creating a writable layer, moving files across layers to enable copy-on-write, etc.? It's the storage drivers.

So Docker uses storage drivers to enable layered architecture. Some of the common storage drivers are aufs, btrfs, devicemapper, overlay, and overlay2.

The selection of the storage driver depends on the underlying OS being used. For example, with Ubuntu, the default storage driver is aufs, whereas this storage driver is not available on other operating systems like Fedora or CentOS. In that case, devicemapper may be a better option. Docker will choose the best storage driver available automatically based on the operating system and the different storage drivers.

Storage drivers also provide different performance and stability characteristics. So you may want to choose one that fits the needs of your application and your organization. If you would like to read more on any of these storage drivers, please refer to the links in the attached documentation.

# Volume Driver Plugins in Docker  
OK, so in the previous lecture we discussed storage drivers. Storage drivers help manage storage on images and containers. We also briefly touched upon volumes in the previous lecture. We learned that if you want to persist storage, you must create volumes.

Remember that volumes are not handled by storage drivers. Volumes are handled by volume driver plugins. The default volume driver plugin is local. The local volume plugin helps create a volume on the Docker host and store its data under the /var/lib/docker/volumes directory.

There are many other volume driver plugins that allow you to create a volume on third-party solutions like Azure File Storage, Convoy, DigitalOcean Block Storage, Docker, Google Compute, Persistent Disks, Cluster of Fastnet, and AppX Ray Port Works. And remember this for storage: these are just a few of the many.

Some of these volume drivers support different storage providers. For instance, the Rexroth storage driver can be used to provision storage on EBS, S3, EMC storage arrays like Isilon and ScaleIO, or Google Persistent Disk, or OpenStack Cinder.

When you run a Docker container, you can choose to use a specific volume driver such as Rexroth’s to provision a volume from Amazon EBS. This will create a container and attach a volume from the cloud. When the container exits, your data is safe in the cloud.

In the upcoming lectures, we will see more about volumes in Kubernetes.

# Volumes  
Before we head into Persistent Volumes, let us start with volumes in Kubernetes. Let us look at volumes in Docker first. Docker containers are meant to be transient in nature, which means they are meant to last only for a short period of time. They're called upon when required to process data and destroyed once finished.

The same is true for the data within the container—the data is destroyed along with the container. To persist data processed by the containers, we attach a volume to the containers when they are created. The data processed by the container is now placed in this volume, thereby retaining it permanently. Even if the container is deleted, the data generated or processed by it remains.

So how does that work in the Kubernetes world? Just as in Docker, the pods created in Kubernetes are transient in nature. When a pod is created to process data and then deleted, the data processed by it gets deleted as well. For this, we attach a volume to the pod. The data generated by the pod is now stored in the volume, and even after the pod is deleted, the data remains.

Let's look at a simple implementation of volumes. We have a single-node Kubernetes cluster. We create a simple pod that generates a random number between one and hundred, and writes that to a file at /opt/number.out. It then gets deleted along with the random number. To retain the number generated by the pod, we create a volume, and a volume needs a storage.

When you create a volume, you can choose to configure its storage in different ways. We will look at the various options in a bit, but for now, we will simply configure it to use a directory on the host. In this case, I specify a path /data on the host. This way, any files created in the volume would be stored in the directory /data on my node.

Once the volume is created, to access it from a container, we mount the volume to a directory inside the container. We use the volumeMounts field in each container to mount the data volume to the directory /opt within the container. The random number will now be written to /opt mounted inside the container, which happens to be on the data volume, which is in fact the data directory on the host. When the pod gets deleted, the file with the random number still lives on the host.

Let's take a step back and look at the volume storage options. We just used the hostPath option to configure it directly on the host as storage space for the volume. Now, that works fine on a single node; however, it is not recommended for use in a multi-node cluster. This is because the pods would use the /data directory on all the nodes and expect all of them to be the same and have the same data. Since they're on different servers, they are in fact not the same unless you configure some kind of external replicated cluster storage solution.

Kubernetes supports several types of different storage solutions, such as NFS, ClusterFS, Flocker, Fiber Channel, CFS, ScaleIO, or public cloud solutions like AWS EBS, Azure Disk or File, or Google Persistent Disk.

For example, to configure an AWS Elastic Block Store volume as the storage option for the volume, we replace the hostPath field of the volume with the AWS Elastic Block Store field along with the volume ID and file system type. The volume storage will now be on AWS EBS.

Well, that's it about volumes in Kubernetes. We will now head over to discuss Persistent Volumes next.

# Persistent Volumes  
In the last lecture, we learned about volumes. Now we will discuss Persistent Volumes in Kubernetes.

When we created volumes in the previous section, we configured volumes within the pod definition file, so every configuration information required to configure storage for the volume goes within the pod definition file.

Now, when you have a large environment with many users deploying lots of pods, the users would have to configure storage every time, for each pod. Whatever storage solution is used, the users who deploy the pods would have to configure that on all pod definition files in their environment. Every time a change is to be made, the user would have to make it on all of their pods.

Instead, you would like to manage storage more centrally. You would like it to be configured in a way that an administrator can create a large pool of storage, and then have users carve out pieces from it as required. That is where Persistent Volumes can help us.

A Persistent Volume (PV) is a cluster-wide pool of storage volumes, configured by an administrator to be used by users deploying applications on the cluster. The users can now select storage from this pool using Persistent Volume Claims (PVCs).

Let us now create a Persistent Volume. We start with the base template and update the API version. Set the kind to PersistentVolume and name it pv-vol-one.

Under the spec section, specify the access modes. Access mode defines how a volume should be mounted on the hosts, whether in a read-only mode or read/write mode, etc. The supported values are ReadOnlyMany, ReadWriteOnce, or ReadWriteMany.

Next is the capacity. Specify the amount of storage to be reserved for this persistent volume, which is set to 1 GB here.

Next comes the volume type. We will start with the hostPath option, which uses storage from the node’s local directory. Remember, this option is not recommended for production environments.

To create the volume, run the kubectl create command. To list the created volume, run the kubectl get persistentvolume command.

You can replace the hostPath option with one of the supported storage solutions, as we saw in the previous lecture, such as AWS Elastic Block Store, etc.

Well, that's it on Persistent Volumes in this lecture. In the next lecture, we will look at how we use Persistent Volume Claims (PVCs) to claim the volume configured with Persistent Volumes.

# Persistent Volume Claims  
In the previous lecture, we created a Persistent Volume (PV). Now we will try to create a Persistent Volume Claim to make the storage available to a node.
Persistent Volumes and Persistent Volume Claims are two separate objects in the Kubernetes namespace. An administrator creates a set of Persistent Volumes, and a user creates Persistent Volume Claims to use the storage.
Once the Persistent Volume Claims are created, Kubernetes binds the Persistent Volumes to claims based on the request and properties set on the volume. Every Persistent Volume Claim is bound to a single Persistent Volume.
During the binding process, Kubernetes tries to find a Persistent Volume that has sufficient capacity as requested by the claim, and matches any other request properties such as access modes, volume modes, storage class, etc.
However, if there are multiple possible matches for a single claim and you want to specifically use a particular volume, you can use labels and selectors to bind to the desired volume.
Finally, note that a smaller claim may get bound to a larger volume if all other criteria match and there are no better options. There is a one-to-one relationship between claims and volumes, so no other claims can utilize the remaining capacity in the volume.
If there are no volumes available, the Persistent Volume Claim will remain in a pending state until new volumes are made available to the cluster. Once new volumes are available, the claim will automatically be bound to the newly available volume.
Let us now create a Persistent Volume Claim. We start with a blank template, set the API version, and set the kind to PersistentVolumeClaim. We will name it my-claim.
Under the specification, set the access modes to ReadWriteOnce and set resources to request 500 MB of storage. Create the claim using the kubectl create command. To view the created claim, run the kubectl get persistentvolumeclaim command.
Initially, we see the claim in a pending state. When the claim is created, Kubernetes looks at the volume created previously. The access modes match, the capacity requested is 500 MB, but the volume is configured with 1 GB of storage. Since there are no other volumes available, the Persistent Volume Claim is bound to the Persistent Volume.
When we run the kubectl get pv command again, we see that the claim is bound to the Persistent Volume we created.
To delete a PVC, run the kubectl delete persistentvolumeclaim command. But what happens to the underlying Persistent Volume when the claim is deleted? You can choose what happens to the volume.
By default, it is set to Retain, meaning the Persistent Volume will remain until it is manually deleted by the administrator. It is not available for reuse by any other claims.
Alternatively, the volume can be deleted automatically, so as soon as the claim is deleted, the volume will be deleted as well, freeing up storage on the end storage device.
A third option is to Recycle. In this case, the data in the volume will be scrubbed before making it available to other claims.
Well, that's it for this lecture. Head over to the coding exercises section and practice configuring and troubleshooting Persistent Volumes and Volume Claims in Kubernetes.

# Using PVCs in Pods  
Using PVCs in Pods
Once you create a PVC use it in a POD definition file by specifying the PVC Claim name under persistentVolumeClaim section in the volumes section like this:
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim

The same is true for ReplicaSets or Deployments. Add this to the pod template section of a Deployment on ReplicaSet.
Reference URL: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#claims-as-volumes

# Note on optional topics  

# Storage Classes  
In the previous lectures, we discussed how to create Persistent Volumes (PVs), then create Persistent Volume Claims (PVCs) to claim that storage, and finally use the PVCs in the pod definition files as volumes.

In that scenario, we create a PVC from a Google Cloud Persistent Disk. The problem here is that before the PV is created, you must have manually created the disk on Google Cloud. Every time an application requires storage, you have to first manually provision the disk on Google Cloud and then manually create a persistent volume definition file using the same name as that of the disk you created. This is called static provisioning of volumes.

It would be nice if the volume gets provisioned automatically when the application requires it. And that's where Storage Classes come in.

With Storage Classes, you can define a provisioner, such as Google Storage, that can automatically provision storage on Google Cloud and attach that storage to pods when a claim is made. This is called dynamic provisioning of volumes.

You do this by creating a Storage Class object, with the API version set to storage.k8s.io/v1. You specify a name and use the provisioner as kubernetes.io/gce-pd.

Going back to our original scenario, where we have a pod using a PVC for its storage, and the PVC is bound to a PV, we now have a Storage Class. So we no longer need the PV definition, because the PV and any associated storage will be created automatically when the Storage Class is used.

For the PVC to use the Storage Class we defined, we specify the storageClassName in the PVC definition. That’s how the PVC knows which storage class to use. Next time a PVC is created, the Storage Class associated with it uses the defined provisioner to provision a new disk with the required size on GCP, then creates a PV, and binds the PVC to that volume.

So remember, it still creates a PV; it’s just that you don’t have to manually create it anymore. It is created automatically by the Storage Class.

In this example, we used the GCE provisioner to create a volume on GCP. There are many other provisioners as well, such as for AWS EBS, Azure File, Azure Disk, CephFS, Portworx, ScaleIO, and so on. With each of these provisioners, you can pass in additional parameters, such as the type of disk to provision, the replication type, etc. These parameters are very specific to the provisioner that you’re using.

For Google Persistent Disk, you can specify the type, which could be Standard or SSD, and you can specify the replication mode, which could be none or regional PD. This allows you to create different storage classes, each using different types of disks.

For example, you could have a silver storage class with standard disks, a gold class with SSD drives, and a platinum class with SSD drives and replication. And that’s why it’s called a storage class—you can create different classes of service. Next time you create a PVC, you can simply specify the class of storage you need for your volumes.

# Why Stateful Sets?  
In this lecture, we will look at Stateful Sets in Kubernetes. Before we talk about stateful sets, we must first understand why we need it. Why can’t we just live with deployments? So let's start from the very basics.

For a minute, let's keep aside everything that we learned so far, such as deployments, Kubernetes, Docker, containers, or virtual machines. Let’s start with a simple server—our good old physical server. Suppose we are tasked to deploy a database server. We install and set up MySQL on the server and create a database. Our database is now operational, and other applications can write data to it.

To withstand failures, we are tasked to deploy a high availability solution, so we deploy additional servers and install MySQL on those as well. We have a blank database on the new servers. How do we replicate that data from the original database to the new databases on the new servers?

For the sake of this lecture, we will use MySQL replication as an example. You don’t need to be a database admin or an expert on MySQL to follow through. We are simplifying the MySQL replication concepts to focus on the high-level procedure and the requirements from a deployment perspective.

There are many ways to replicate a database, and we will discuss one: the single master, multi-slave topology. In this topology, all writes come into the master server, and reads can be served by either the master or any of the slave servers. The master server should be set up first before deploying the slaves. Once the slaves are deployed, perform an initial clone of the database from the master to the first slave. After the initial copy, enable continuous replication from the master to that slave, so the database on the slave node is always in sync with the master.

For the second slave, instead of cloning data directly from the master (which can impact resources), it is better to clone data from slave one once it is ready, and then enable continuous replication from the master to slave two. Note that both slaves are configured with the address of the master host, so they know where the master is.

To summarize the process: we want the master to come up first, then slave one, clone data from master to slave one, enable replication, wait for slave one to be ready, clone data from slave one to slave two, enable continuous replication from master to slave two, and ensure that slaves are configured with the master’s address. This is the high-level plan for deploying a MySQL cluster.

Now, let’s map this to Kubernetes and containers. In Kubernetes, each of these instances—including the master and slaves—is a pod, part of a deployment. This allows scaling up or down as required.

However, in step one, we want the master to come up first and then the slaves. With deployments, all pods come up at the same time, so we cannot guarantee the order. Additionally, during cloning and replication, we must differentiate between master and slave pods. Deployments give pods random names, which means a crashed master pod will get a new name, breaking replication configuration.

This is where Stateful Sets come in. Stateful sets are similar to deployments in that they create pods based on a template, can scale up or down, and support rolling updates and rollbacks. However, with Stateful Sets, pods are created in a sequential order. After the first pod is deployed, it must be in a running and ready state before the next pod is deployed. This ensures that the master comes up first, then slave one, then slave two.

Stateful sets assign a unique ordinal index to each pod, starting from zero. Each pod gets a unique name derived from this index combined with the stateful set name: for example, mysql-0, mysql-1, mysql-2, and so on. These names are predictable and stable, unlike deployment pod names.

This allows you to designate mysql-0 as the master. mysql-1 knows it should clone data from mysql-0, and mysql-2 can clone from mysql-1. If you scale up, the new pod mysql-3 can clone from mysql-2. Continuous replication can point all slaves to mysql-0. Even if the master fails and is recreated, it retains the same name. Stateful sets maintain a sticky identity for each pod, ensuring master-slave relationships remain intact.

In summary, Stateful Sets are necessary for workloads that require ordered deployment, stable network identities, and persistent storage. They solve the challenges we face with database clusters and other stateful applications.

# Stateful Sets Introduction  
In this lecture, we will talk about StatefulSets in Kubernetes. Now, in the previous lecture, we discussed why you need a StatefulSet. Note that you might not always need a StatefulSet. It really depends on the kind of application you’re trying to deploy.

If the instances need to come up in a particular order, or if the instances need a stable name, etc., then a StatefulSet may be required. Once you evaluate your requirements and decide that a StatefulSet is the right choice for you, you can create a StatefulSet just like how you created a deployment.

You create a deployment definition file with a pod definition template inside it. All you need to do is change the kind to StatefulSet instead of Deployment. Note that both letters SS in StatefulSet are uppercase.

Apart from that, a StatefulSet also requires a service name to be specified. You must specify the name of a headless service. We will discuss headless services and why you need to specify a service name in the upcoming lecture.

As discussed before, when you create a StatefulSet using this definition file, it creates pods one after the other. This is called ordered graceful deployment. Each pod gets a stable, unique DNS record on the network that any other application can use to access the pod.

When you scale the StatefulSet, it scales in an ordered graceful fashion, where each pod comes up, becomes ready, and only then the next one comes up. This helps when you want to scale MySQL databases, as each new instance can clone from the previous instance.

It works in reverse order when you scale it down. The last instance is removed first, followed by the second-last one. The same is true on termination: when you delete a StatefulSet, the pods are deleted in reverse order.

Now, that’s the default behavior of a StatefulSet, but you can override this behavior to cause a StatefulSet to not follow an ordered launch while still retaining other benefits such as a stable and unique network ID. For that, you could set the pod management policy field to Parallel to instruct the StatefulSet to deploy all pods in parallel. The default value of this field is OrderedReady.

# Headless Services
  Let us now look at headless services. In the previous lecture, we said that when you create a StatefulSet, it deploys one pod at a time, assigns an ordinal index, and that each pod has a stable, unique name, such as mysql-0, mysql-1, and mysql-2. This allows us to point the slave pods to reach the master at mysql-0.

However, there is something missing here. From what we know about services and DNS in Kubernetes, the way you point one application within the cluster to another application is through a service. For example, if we had a web server that needs to access the database server, we would create a service for the database and name it mysql.

The service acts as a load balancer. Traffic coming into the service is balanced across all the pods in the deployment. The service has a ClusterIP and a DNS name associated with it, usually in the format mysql.default.svc.cluster.local. Any other application within the environment, like a web server, can use this DNS name to reach the MySQL database.

Now, remember in a master-slave topology, reads can be served by the master or slaves, but writes must only go to the master. It is okay for the web server to read from the mysql service, but you cannot write to it, as the service will load balance the writes to all pods under it, which is not desirable for a MySQL cluster. We need a way to point the web server to the master pod only.

One way to reach the master pod directly is to use its IP address, but IPs are dynamic and may change if the pod is recreated, so that is not reliable. Each pod can also be reached via its DNS address, but this DNS is based on the pod’s IP, which changes, so that also doesn’t help. What we need is a service that does not load balance requests but provides a DNS entry for each pod. This is what a headless service is.

A headless service is created like a normal service but does not have a ClusterIP. It does not perform any load balancing. Its only purpose is to create DNS entries for each pod using the pod name and a subdomain. For example, if we create a headless service named mysql-h, each pod gets a DNS record in the form pod-name.headless-service.namespace.cluster-domain. In this case, the master pod can be accessed at mysql-0.mysql-h.default.svc.cluster.local.

To create a headless service, you define a service as usual with API version v1 and kind Service. Name it mysql-h for the headless service, specify ports, and the selector for the pods. The key difference is setting ClusterIP to None.

When creating the pods, the DNS entries for individual pods are only created if two conditions are met in the pod spec: hostname and subdomain. The subdomain must match the headless service name. Specifying hostname ensures a DNS record is created for the pod itself.

If you deploy pods as part of a deployment and do not specify hostname or subdomain, the headless service does not create A records for the pods. If you specify these fields in a deployment, all pods get the same hostname and subdomain, resulting in DNS records like mysql-pod.mysql-h.default.svc.cluster.local for all pods, which does not allow individual pod addressing.

This is where StatefulSets differ from deployments. When creating a StatefulSet, you do not need to manually set hostname or subdomain. The StatefulSet automatically assigns the correct hostname for each pod based on its name and the correct subdomain based on the headless service.

But how does a StatefulSet know which headless service to use? When creating the StatefulSet, you must explicitly specify the service name in the StatefulSet definition file. The StatefulSet then uses this service name to assign the subdomain property to each pod, ensuring that each pod gets a unique DNS record.

# Storage in StatefulSets  
In this lecture, we will talk about storage in StatefulSets. Before we begin, let me quickly recap what we already know about storage in Kubernetes. It’s only going to take a minute, so bear with me, and I think this is important.

Earlier, we discussed persistent volumes (PVs). We create volume objects in Kubernetes, which are then claimed by persistent volume claims (PVCs) and finally used in pod definition files within pods. This maps a single persistent volume to a single persistent volume claim to a single pod definition file.

With dynamic provisioning, using a storage class definition, we remove the need for manually creating persistent volumes and instead use storage provisioners to automatically provision volumes on cloud providers. The PV is created automatically, but we still create the PVC manually and associate it with a pod. This works fine for a pod with a volume.

So how does this change with deployments or StatefulSets? With StatefulSets, when you specify the same PVC under the pod definition, all pods created by that StatefulSet try to use the same volume. This is possible if you want multiple pods or instances of your application to share the same storage. This setup depends on the kind of volume created and the provisioner used. Not all storage types support read/write from multiple instances simultaneously.

But what if you want separate volumes for each pod, like in the MySQL replication use case we’ve been discussing? The pods don’t want to share data. Instead, each pod needs its own local storage. Each instance has its own database, and replication occurs at the database level. In this case, each pod needs its own PVC, which is bound to a PV. These PVs can be created from a single or multiple storage classes.

How do you automatically create a PVC for each pod in a StatefulSet? This can be achieved using a volume claim template. A volume claim template is essentially a persistent volume claim in template form. Instead of creating a PVC manually and then specifying it in the StatefulSet definition, you move the entire PVC definition into a section named volumeClaimTemplates under the StatefulSet specification. volumeClaimTemplates is an array, so you can specify multiple templates if needed.

Here’s how it works: We have a StatefulSet with volume claim templates and a storage class definition with the correct provisioner for GCE. When the StatefulSet is created, it first creates the first pod, and during its creation, a PVC is created. The PVC is associated with a storage class, which provisions a volume on GCP, creates a PV, and binds the PVC to the PV. Then the second pod is created, and a new PVC is provisioned, bound to a new PV, and so on for subsequent pods.

What happens if one of these pods fails and is recreated or rescheduled onto a different node? StatefulSets do not automatically delete the PVC or the associated volume. Instead, they ensure that the pod is reattached to the same PVC it was originally using. This ensures stable storage for pods in StatefulSets.

Well, that’s it for now, and thank you so much for listening.
