# Define, build and modify container images
Hello, and welcome to this lecture on Docker images. In this session, we will learn how to create our own Docker images and understand why this is necessary. You might need a custom image if the application or component you require is not available on Docker Hub, or if your team decides to containerize an application for easier shipping and deployment. In this example, we will containerize a simple web application built using the Python Flask framework.

Before creating the image, it is important to understand the application and its dependencies. If deploying manually, we would start with an operating system like Ubuntu, update the package repositories, install system dependencies, install Python packages via pip, copy the application source code to a location such as /opt, and finally run the web server using the Flask command. These steps form the basis of the Dockerfile instructions.

A Dockerfile is a text file containing instructions Docker understands. Each line consists of an instruction and its arguments. Common instructions include FROM, RUN, COPY, and ENTRYPOINT. The FROM instruction specifies the base image (e.g., Ubuntu), which every Docker image must have. The RUN instruction executes commands inside the container, such as installing packages. The COPY instruction transfers files from the local system into the image, like moving the application source code to /opt/app. Finally, ENTRYPOINT defines the command that runs when a container is started from the image.

Docker builds images in a layered architecture, where each instruction creates a new layer containing only the changes from the previous layer. For example, the first layer is the base Ubuntu OS, the second layer installs system packages, the third layer installs Python packages, the fourth layer copies the source code, and the fifth layer sets the entry point. This layered approach has several benefits: it reduces the size of the image by storing only incremental changes, speeds up rebuilds by caching previous layers, and allows restarting the build from a failed step instead of starting over. Only layers above the changes need to be rebuilt when modifying the Dockerfile or application code.

Once the Dockerfile is ready, you can build the image using docker build with a tag name. This creates a local image on your system. To share the image publicly, use docker push with your Docker Hub account and image name. For example, moonshot/mycustomapp. You can also inspect image layers and their sizes using docker history.

Almost anything can be containerized, from databases and development tools to browsers and utilities like curl, or even applications like Spotify and Skype. Containerization ensures that applications run consistently across environments, simplifies deployment, and allows easy cleanup without leaving residual files. Going forward, Docker and containerized applications will become the standard way of running software.


# Commands and Arguments in Docker
Hello, and welcome to this lecture on commands and arguments in Docker containers. My name is Mumshad Mannambeth, and in this session, we will discuss how commands, arguments, and entry points work in Docker, which is an important concept often overlooked when learning Kubernetes pod definitions. To start, letâ€™s recall how commands work in Docker. When you run a container from an image, such as using docker run ubuntu, Docker creates a container and starts the default process defined in the image. For example, the Ubuntu image uses Bash as its default command. Since Bash expects a terminal to interact with and Docker does not attach one by default, the container immediately exits. Unlike virtual machines, containers are meant to run a specific task or process, such as a web server, database, or computation, and they only live as long as that process runs.

The default process for an image is defined using the CMD instruction in the Dockerfile. For instance, the nginx image uses the nginx command, and the MySQL image uses the mysql command. If you want to run a different command temporarily, you can append it to the docker run command, such as docker run ubuntu sleep 5, which overrides the default CMD and runs the container for five seconds. To make this change permanent, you can create a new image from the base image and specify the new command in the Dockerfile. Commands can be specified in shell form or in array (exec) format, where the first element is the executable and the following elements are parameters. This separation is important for proper execution.

When you build the new image, for example, naming it ubuntu-sleeper, running it will execute the specified command automatically. But if you want flexibility, such as changing the sleep duration without modifying the image, you can use the ENTRYPOINT instruction. ENTRYPOINT defines the default executable, while any parameters provided at runtime get appended. This is different from CMD, where runtime parameters completely replace the CMD instruction. Using both ENTRYPOINT and CMD together allows you to set a default value while still allowing overrides. For example, the container can default to sleep 5 but accept a different number of seconds when you run docker run ubuntu-sleeper 10. You can even override the entry point at runtime using the --entrypoint option if needed.

Understanding commands, arguments, and entry points in Docker is crucial because it directly translates to defining commands and arguments in Kubernetes pod definitions, which we will explore in the next lecture.

# Commands and Arguments in Kubernetes
Hello and welcome to this lecture. In this session, we will discuss commands and arguments in a Kubernetes pod. In the previous lecture, we created a simple Docker image called Ubuntu Sleeper, which sleeps for a specified number of seconds. By default, it sleeps for five seconds, but this can be overridden by passing a command-line argument when running the container with docker run ubuntu-sleeper. Now, we will see how to do the same in a Kubernetes pod. We start by creating a blank pod definition file, provide a name for the pod, and specify the image to use. When this pod is created, it instantiates a container from the specified image, which sleeps for five seconds before exiting.

If you want the container to sleep for a different duration, such as 10 seconds, you can specify the additional argument in the pod definition using the args field. This field takes an array of values that are appended to the default command in the image. To relate this to the Dockerfile we created earlier, remember that the Dockerfile has both ENTRYPOINT and CMD instructions. The ENTRYPOINT defines the command to run at container startup, while CMD provides default parameters. In Kubernetes, the args field overrides the CMD instruction in the Dockerfile, while the command field in the pod definition overrides the ENTRYPOINT. For example, if you wanted to override the entry point from sleep to a hypothetical sleep2.0 command, you would use the command field in the pod specification.

In summary, Kubernetes provides two separate fields in a pod definition that correspond to Dockerfile instructions: the command field overrides ENTRYPOINT, and the args field overrides CMD. It is important to remember that the command field does not override CMD. This distinction ensures precise control over what process runs in your containers and how arguments are passed at runtime.

# Quick note on editing pod and deployment
A quick note on editing Pods and Deployments
Edit a POD
Remember, you CANNOT edit specifications of an existing POD other than the below.
spec.containers[*].image
spec.initContainers[*].image
spec.activeDeadlineSeconds
spec.tolerations

For example you cannot edit the environment variables, service accounts, resource limits (all of which we will discuss later) of a running pod. But if you really want to, you have 2 options:

1. Run the kubectl edit pod <pod name> command.  This will open the pod specification in an editor (vi editor). Then edit the required properties. When you try to save it, you will be denied. This is because you are attempting to edit a field on the pod that is not editable.

A copy of the file with your changes is saved in a temporary location as shown above.
You can then delete the existing pod by running the command:
kubectl delete pod webapp

Then create a new pod with your changes using the temporary file
kubectl create -f /tmp/kubectl-edit-ccvrq.yaml

2. The second option is to extract the pod definition in YAML format to a file using the command
kubectl get pod webapp -o yaml > my-new-pod.yaml
Then make the changes to the exported file using an editor (vi editor). Save the changes
vi my-new-pod.yaml
Then delete the existing pod
kubectl delete pod webapp
Then create a new pod with the edited file
kubectl create -f my-new-pod.yaml

Edit Deployments
With Deployments you can easily edit any field/property of the POD template. Since the pod template is a child of the deployment specification,  with every change the deployment will automatically delete and create a new pod with the new changes. So if you are asked to edit a property of a POD part of a deployment you may do that simply by running the command

kubectl edit deployment my-deployment 

# Envirnment Variable
Hello. In this lecture, we will learn how to set environment variables in Kubernetes using a pod definition file, with the same image we used in the previous Docker lecture. To define environment variables, we use the env property in the pod specification. The env field is an array, so each item begins with a dash (-) and includes two properties: name, which is the name of the environment variable available to the container, and value, which is the corresponding value. This is the direct method of specifying environment variables using simple key-value pairs.

Kubernetes also provides more advanced ways to manage environment variables, such as using ConfigMaps and Secrets. Instead of specifying the value directly, you can use valueFrom to reference data from a ConfigMap or Secret. This approach helps decouple configuration and sensitive information from the pod definition, making your deployments more flexible and secure. ConfigMaps and Secrets will be discussed in detail in upcoming lectures.

# ConfigMap
In this lecture, we will discuss how to manage configuration data in Kubernetes using ConfigMaps. In the previous lecture, we learned how to define environment variables directly in a pod definition file. However, when you have multiple pods, managing environment data within each pod file becomes cumbersome. ConfigMaps allow you to centralize configuration and inject it into pods, making key-value pairs available as environment variables inside containers.

There are two steps to using ConfigMaps: creating the ConfigMap and injecting it into a pod. ConfigMaps can be created in two ways: imperatively (without a file) or declaratively (using a definition file). In the imperative approach, you use the kubectl create configmap command with the --from-literal option to directly specify key-value pairs, for example, creating a ConfigMap named app-config with a key app-color=blue. You can also use --from-file to read configuration data from a file, which is useful for larger configurations.

In the declarative approach, you create a YAML definition file with apiVersion: v1, kind: ConfigMap, metadata with the ConfigMap name, and a data section containing key-value pairs. After creating the file, running kubectl create -f <filename> will create the ConfigMap. Multiple ConfigMaps can be created for different purposes, such as one for your application, one for MySQL, and another for Redis. Itâ€™s important to give them meaningful names, as these names are used later when associating them with pods. You can view ConfigMaps using kubectl get configmaps and inspect their contents with kubectl describe configmaps <name>.

To inject a ConfigMap into a pod, use the envFrom property under the container specification. This property is a list where each item refers to a ConfigMap by name, making all its key-value pairs available as environment variables in the container. This approach allows your pod, for example, to configure a web application with a blue background dynamically based on the ConfigMap.

Besides injecting configuration as environment variables, ConfigMaps can also be mounted as files in a volume, providing additional flexibility. The coding exercises accompanying this lecture will let you practice creating, configuring, and troubleshooting ConfigMaps and environment variables in a live Kubernetes environment.

# Secrets
Secrets in Kubernetes
Secrets are used to store sensitive information such as passwords, API keys, or tokens. Unlike ConfigMaps, Secrets are encoded (base64) instead of plain text.
Why use Secrets?
* Avoid hardcoding sensitive data in code or pod definitions.
* Protect passwords and keys from accidental exposure.

Two Steps to Use Secrets:
1. Create the Secret
2. Inject it into Pods

Creating Secrets:
1. Imperative Method:
   Use `kubectl create secret` with `--from-literal`:
kubectl create secret generic app-secret 
--from-literal=DB_Host=mysql 
--from-literal=DB_User=root 
--from-literal=DB_Password=pass123

* Use `--from-file` to read Secret data from a file.

2. Declarative Method:
   Create a YAML file:

apiVersion: v1
kind: Secret
metadata:
name: app-secret
data:
DB_Host: bXlzcWw=       # base64 encoded
DB_User: cm9vdA==       # base64 encoded
DB_Password: cGFzczEyMw==  # base64 encoded
    * Encode values with: `echo -n "value" | base64`
    * Decode with: `echo -n "<encoded_value>" | base64 --decode`
    * Apply YAML: `kubectl create -f secret.yaml`
Viewing Secrets:
    * List Secrets: `kubectl get secrets`
    * Describe Secret: `kubectl describe secret app-secret`
    * View encoded values: `kubectl get secret app-secret -o yaml`
Injecting Secrets into Pods:
1. As Environment Variables:
envFrom:
* secretRef:
  name: app-secret
2. As Files in a Volume:
volumes:
* name: secret-volume
  secret:
  secretName: app-secret
* Each key becomes a file in the volume, with the value as content.
Important Considerations:
1. Secrets are only encoded, not encrypted.
2. Do not commit Secret definition files to version control.
3. ETCD stores Secrets in plain text by default. Consider enabling encryption at rest.
4. Any user able to create pods/deployments in the same namespace can access Secrets. Use RBAC to limit access.
5. External Secret Providers (AWS, Azure, GCP, Vault) can store Secrets securely outside ETCD.

# A quick note about Secrets!
Remember that secrets encode data in base64 format. Anyone with the base64 encoded secret can easily decode it. As such the secrets can be considered as not very safe.
The concept of safety of the Secrets is a bit confusing in Kubernetes. The kubernetes documentation page and a lot of blogs out there refer to secrets as a "safer option" to store sensitive data. They are safer than storing in plain text as they reduce the risk of accidentally exposing passwords and other sensitive data. In my opinion it's not the secret itself that is safe, it is the practices around it. 
Secrets are not encrypted, so it is not safer in that sense. However, some best practices around using secrets make it safer. As in best practices like:
Not checking-in secret object definition files to source code repositories.
Enabling Encryption at Rest for Secrets so they are stored encrypted in ETCD. 

Also the way kubernetes handles secrets. Such as:
A secret is only sent to a node if a pod on that node requires it.
Kubelet stores the secret into a tmpfs so that the secret is not written to disk storage.
Once the Pod that depends on the secret is deleted, kubelet will delete its local copy of the secret data as well.
Read about the protections and risks of using secrets here

Having said that, there are other better ways of handling sensitive data like passwords in Kubernetes, such as using tools like Helm Secrets, HashiCorp Vault. I hope to make a lecture on these in the future.

importand link :-
https://kubernetes.io/docs/concepts/configuration/secret/
https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/
https://kubernetes.io/docs/concepts/configuration/secret/#risks
https://kubernetes.io/docs/concepts/configuration/secret/#protections
https://developer.hashicorp.com/vault

# Additional Resource
Dive deep into the world of Kubernetes security with our comprehensive guide to Secret Store CSI Driver.
https://www.youtube.com/watch?v=MTnQW9MxnRI

# Demo: Encrypting Secret Data at Rest
Absolutely! Here's a plain text diagram that visually explains the encryption flow of secrets at rest in Kubernetes, from the moment you create a secret to how itâ€™s encrypted and stored in ETCD.

* Kubernetes Secret Encryption at Rest Flow
==========================================
1. Create a Secret
------------------
   [kubectl CLI]
       |
       v
   kubectl create secret my-secret --from-literal=password=admin123
       |
       v
2. API Request Sent to kube-apiserver
-------------------------------------
   [kube-apiserver]
   - Validates and processes the request
   - Checks EncryptionConfiguration for the 'secrets' resource
       |
       v
3. Encryption (if enabled)
--------------------------
   [EncryptionConfiguration]
   resources:
     - secrets
       providers:
         - aescbc:
             keys:
               - name: key1
                 secret: <base64-encoded-key>
         - identity: {}
   --> kube-apiserver encrypts the secret data using the first provider (e.g., aescbc)
       |
       v
4. Store Encrypted Secret in ETCD
---------------------------------
   [ETCD (Key-Value Store)]
   Key: /registry/secrets/default/my-secret
   Value: <ENCRYPTED_BINARY_BLOB>
   ðŸ”’ Data is encrypted at rest â€” NOT base64-encoded, but actually encrypted.
       |
       v
5. Retrieve Secret (kubectl get secret)
---------------------------------------
   [kube-apiserver]
   - Decrypts secret using the same EncryptionConfiguration
   - Sends decrypted data back to the user
       |
       v
   [kubectl CLI]
   - Shows base64-encoded plaintext (user-readable after decoding)

* Important Notes:
Only secrets (and optionally configmaps, PVCs, etc.) defined in the encryption config are encrypted.
Only new or updated secrets are encrypted â€” old secrets must be manually re-saved to be encrypted.
The encryption key must be kept secure. If compromised, encrypted data can be decrypted.

# Pre-requisite - Security in Docker
In this lecture, we discuss security context in Kubernetes, starting with some foundational concepts in Docker security. Containers, unlike virtual machines, are not fully isolated from the hostâ€”they share the same kernel. Containers achieve isolation through Linux namespaces, which means that each container sees only its own processes, while the host can see all container processes as just another process in the system. This process isolation allows containers to operate independently while running on the same host.

By default, Docker runs container processes as the root user. This can be verified both inside the container and on the host. If you prefer not to run processes as root, you can specify a user ID using the --user option in the docker run command. Alternatively, the user can be set permanently within the Docker image itself using the USER instruction. When a container runs under a specific user ID, its processes will run with those privileges instead of root.

It is important to understand that the root user within a container does not have the same privileges as the root user on the host. Docker enforces security through Linux capabilities, limiting what the containerâ€™s root user can do. Without these restrictions, a container running as root could perform highly privileged operations such as modifying files, creating or killing processes, binding to network ports, changing system configurations, or even rebooting the host. By default, Docker provides a restricted set of capabilities, protecting the host and other containers from unintended actions.

If needed, you can modify these capabilities. For example, you can grant additional privileges using --cap-add, remove certain privileges using --cap-drop, or run a container with full privileges using the --privileged flag. Understanding these security mechanisms is essential to running containers safely, and Kubernetes builds upon these concepts to define security context at the pod and container level, which we will explore in the next lecture.

# Security context
In this lecture, we discuss security context in Kubernetes. As we saw in the previous lecture, when running a Docker container, you can define a set of security settings such as the user ID for running the container and the Linux capabilities that can be added or removed. These same configurations can be applied in Kubernetes, where containers are encapsulated within pods. You can configure security settings either at the pod level or at the container level. When configured at the pod level, the settings automatically apply to all containers within that pod. However, if security settings are defined at both the pod and container level, the container-level settings override the pod-level configuration.

To configure security context in Kubernetes, start with a pod definition file. For example, a pod running an Ubuntu image with a sleep command can be used to demonstrate security settings. Under the specs section of the pod, add a securityContext field and use the runAsUser option to specify the user ID for the pod. To apply the same configuration at the container level, move the securityContext section under the container specification. Additionally, you can configure Linux capabilities using the capabilities option and specify a list of capabilities to add to the pod or container.

Understanding and configuring security context is crucial for controlling the privileges of containers and ensuring the security of your Kubernetes cluster. You can now practice viewing, configuring, and troubleshooting security contexts in pods and containers to gain hands-on experience.

# Resource requirment
In this lecture, we discuss resource requirements in Kubernetes. Consider a three-node Kubernetes cluster, where each node has a set of CPU and memory resources available. Every pod requires a certain amount of resources to run; for example, a pod may require two CPUs and one unit of memory. When a pod is placed on a node, it consumes the available resources on that node. The Kubernetes scheduler decides which node a pod should be placed on, taking into account the resources requested by the pod and those available on each node. If a node has insufficient resources, the scheduler avoids placing the pod there and looks for another node with sufficient resources. If no node has adequate resources, the pod remains in a pending state, and events such as "insufficient CPU" can be seen using kubectl describe pod.

Resource requirements for a pod are specified as requests and limits. The requests define the minimum CPU and memory required for a container, which the scheduler uses to identify a suitable node. For instance, a container could request one CPU and one gibibyte of memory. In a pod definition file, this is specified under the resources section with requests. When a pod is placed on a node, it is guaranteed the requested resources. CPU can be specified in units as low as 0.1 (100m), and one CPU typically corresponds to one vCPU in AWS, one core in GCP or Azure, or one hyperthread on other systems. Memory can be specified using suffixes like Mi for mebibytes or Gi for gibibytes. It is important to note that G refers to gigabytes (1,000 MB) and Gi to gibibytes (1,024 MiB).

By default, containers have no limits on resource consumption. If a container starts with one CPU, it can consume more as required, potentially affecting other processes or containers on the node. Limits restrict the maximum resources a container can consume. For example, a limit of one CPU and 512 MiB memory ensures that a container cannot exceed these values. Limits and requests are set per container, and multiple containers in a pod can have individual resource specifications. When a container exceeds CPU limits, it is throttled; for memory, exceeding limits results in pod termination, producing an OOM (Out of Memory) error.

There are different scenarios for requests and limits. Without requests or limits, a pod can consume all node resources, potentially starving other pods. If limits are set but requests are not, Kubernetes automatically sets requests equal to limits. When both requests and limits are set, each pod is guaranteed its request but can use resources up to the limit if available. The most flexible approach is to set requests without limits: the pod is guaranteed the requested resources but can use extra available resources when needed. Setting limits may be necessary in shared or public environments to prevent misuse, such as cryptocurrency mining, but in private clusters, you might choose not to set limits while ensuring requests are defined for all pods to guarantee resources.

The same principles apply to memory, but unlike CPU, memory cannot be throttled. If a pod exceeds memory limits, it is terminated to free resources. By default, Kubernetes does not enforce resource requests or limits, which can lead to resource starvation if some pods consume all available resources. To ensure defaults, LimitRanges can be defined at the namespace level. A LimitRange specifies default requests, limits, maximum, and minimum values for CPU and memory. These limits apply only to newly created pods and do not affect existing ones.

To control total resource usage across all pods in a namespace, ResourceQuotas can be used. These namespace-level objects set hard limits for total CPU and memory requests and consumption. For example, a quota can limit the total requested CPU to four vCPUs and memory to four GiB, with maximum limits set for all pods combined. This ensures cluster resources are distributed according to defined policies and prevents resource exhaustion.

In summary, understanding and configuring resource requests, limits, LimitRanges, and ResourceQuotas is critical to maintaining stable and fair resource usage in Kubernetes clusters. These configurations ensure that pods receive guaranteed resources while also protecting the node and cluster from overconsumption.

# Service Account
In this lecture, we discuss service accounts in Kubernetes. Service accounts are linked to other security concepts such as authentication, authorization, and role-based access control, but for application developers, it is sufficient to understand how to create and use them. Kubernetes has two types of accounts: user accounts and service accounts. User accounts are for humans, such as administrators or developers, whereas service accounts are for machines or applications. For instance, monitoring tools like Prometheus use service accounts to pull data from the Kubernetes API, and automated build tools like Jenkins use them to deploy applications on the cluster.

To illustrate, consider a simple Kubernetes dashboard application built in Python, which retrieves a list of pods from the Kubernetes API. The application needs to be authenticated, and this is done using a service account. A service account is created using the command kubectl create serviceaccount <name> (for example, dashboard-sa). Viewing service accounts can be done using kubectl get serviceaccount. When a service account is created, a token is automatically generated and stored in a secret object (for example, dashboard-sa-token-kbbdm). This token is used as a bearer token to authenticate external applications to the Kubernetes API, such as via cURL or within the application itself.

If the application runs inside the Kubernetes cluster, the service account token can be automatically mounted as a volume inside the pod, making it accessible without manual configuration. Every namespace has a default service account, which is automatically mounted to pods if no other service account is specified. The token is mounted under /var/run/secrets/kubernetes.io/serviceaccount, where three files are created, including one named token that contains the authentication token. The default service account has limited permissions, allowing only basic API queries. To use a custom service account, the pod definition file must be updated to specify the service account name. Existing pods cannot have their service accounts changed, but deployments can update the service account, triggering a new rollout of pods. Automatic mounting of service account tokens can be disabled by setting automountServiceAccountToken: false in the pod spec.

Recent Kubernetes versions (1.22 and 1.24) introduced changes to improve security and scalability of service account tokens. Previously, each service account automatically generated a secret with a token that had no expiry and was not audience-bound, which could lead to security risks and scalability issues. Version 1.22 introduced the TokenRequest API, which generates tokens that are time-bound, audience-bound, and object-bound. Tokens for pods are now created via this API and mounted as projected volumes, rather than relying on static secret objects. Version 1.24 further enhanced this by removing automatic creation of secret objects when a service account is created. Tokens must now be generated using kubectl create token <serviceaccount>, which provides a token with a default expiry (usually one hour) and can be extended with additional options.

If needed, the old method of creating non-expiring token secrets is still possible by creating a secret object with type: kubernetes.io/service-account-token and specifying the service account in annotations. However, this is not recommended unless the TokenRequest API cannot be used and the security exposure of a persistent token is acceptable. Overall, the TokenRequest API is the preferred method for generating service account tokens, as it provides greater security and a bounded lifetime compared to the legacy token secrets.

To learn more, refer to the Kubernetes Enhancement Proposals (KEPs) and the official documentation on service accounts and secrets.

# Taints and Tolerations
In this lecture, we discuss taints and tolerations in Kubernetes, which help control the pod-to-node placement relationship. Taints and tolerations can be a bit confusing at first, so an analogy is useful: imagine a person sprayed with bug repellent (a taint). Bugs that are intolerant to the smell will be repelled, while bugs that are tolerant will land regardless. Translating this to Kubernetes, the person is a node and the bugs are pods. Taints are applied to nodes, and tolerations are applied to pods to allow them to be scheduled on nodes with specific taints. Taints and tolerations do not affect security; they simply restrict which pods can be scheduled on which nodes.

Consider a cluster with three worker nodes (node one, two, and three) and four pods (A, B, C, and D). Without any restrictions, the scheduler balances pods across the nodes. If node one has dedicated resources for a specific application, a taint can be applied to prevent other pods from being scheduled there. For example, a taint with key app=blue prevents all pods without a corresponding toleration from being scheduled on node one. To allow a particular pod (say, pod D) to be scheduled on node one, a toleration matching the taint must be added to that pod. The scheduler will then successfully place pod D on node one, while pods A, B, and C are placed on other nodes.

Taints are applied using the command kubectl taint nodes <node> <key>=<value>:<effect>. There are three taint effects: NoSchedule (pods that do not tolerate the taint will not be scheduled on the node), PreferNoSchedule (the scheduler tries to avoid placing pods on the node but does not guarantee it), and NoExecute (new pods not tolerating the taint will not be scheduled, and existing pods that do not tolerate it will be evicted). Tolerations are added in the pod definition file under the spec.tolerations section, matching the taint key, value, operator, and effect. Once applied, the pods will either be scheduled accordingly or evicted based on the taint effect.

For example, if node one is tainted with app=blue:NoExecute to dedicate it for a special application, pods that do not tolerate this taint will be evicted (killed), while pod D, which tolerates the taint, continues to run. It is important to note that taints and tolerations do not guarantee a pod will be scheduled on a specific node; they only restrict which pods a node will accept. If you need to force a pod to a specific node, you would use node affinity, which is a different concept.

Finally, a common point of interest is the master node in a Kubernetes cluster. Although the master node has the capacity to host pods, it is automatically tainted during cluster setup to prevent scheduling application workloads on it. This ensures the master node is dedicated to running cluster management components. You can view this taint using kubectl describe node <master-node> and inspect the taint section. It is generally considered best practice not to schedule workloads on master nodes.

Overall, taints and tolerations allow fine-grained control over which pods can run on which nodes. They are applied at the node and pod level, respectively, and are useful for isolating workloads or dedicating resources to specific applications.

# Node Selector logging
In this lecture, we will talk about nodeSelectors in Kubernetes. Imagine a cluster with three nodes: two smaller nodes with limited resources and one larger node with higher capacity. Different workloads run across this cluster, but the data processing jobs that require more resources should ideally run only on the larger node. In the default setup, however, the Kubernetes scheduler may place these workloads on any node, which could cause problems if a pod lands on a smaller node. For example, Pod C might end up on node two or three, even though only node one has the capacity to handle it. To prevent this, we can restrict certain pods to specific nodes.

The simplest way to achieve this is by using a nodeSelector. In the pod definition file, we can add a nodeSelector field under the spec section and assign it a key-value pair, such as size: large. But where does this key-value pair come from? Kubernetes relies on labels assigned to nodes. These labels help the scheduler identify which nodes meet the conditions for placing the pod. Just like labels and selectors in services, replica sets, and deployments, node labels work the same way here.
* To label a node, we use the command:- kubectl label nodes <node-name> <key>=<value>
* For example:- kubectl label nodes node1 size=large
This labels node one with size=large. After labeling, when we create a pod with nodeSelector: { size: large }, the scheduler will place it on node one, as desired.
While nodeSelectors are simple and effective, they are limited. They only allow for straightforward, single-label matching. For instance, you cannot express conditions like "place the pod on large or medium nodes" or "place the pod on any nodes that are not small." To handle such complex requirements, Kubernetes introduced node affinity and anti-affinity, which provide more flexibility and expressive rules for pod placement. We will explore those concepts in the next lecture.

# Node affinity
In this lecture, we will discuss the node affinity feature in Kubernetes. The main purpose of node affinity is to ensure that pods are scheduled on specific nodes, such as making sure a large data processing pod is placed on a high-capacity node. In the previous lecture, we achieved this using nodeSelectors, but as we saw, nodeSelectors are limited since they cannot handle advanced conditions like logical OR or NOT. Node affinity addresses this limitation by providing more expressive rules, though at the cost of added complexity. For example, a simple nodeSelector configuration to place a pod on a large node becomes a longer specification under node affinity, but both achieve the same outcome.
The node affinity configuration sits under the spec â†’ affinity â†’ nodeAffinity section in the pod definition file. Within nodeAffinity, you specify rules under properties like requiredDuringSchedulingIgnoredDuringExecution. Inside this, you define node selector terms, which include key-value pairs expressed as key, operator, and values. The In operator ensures that a pod is scheduled only on nodes whose labels match one of the given valuesâ€”for example, size In [large]. You can also provide multiple values, such as large or medium. Similarly, the NotIn operator excludes certain labels, for example, size NotIn [small]. If you want to check only whether a label exists, you can use the Exists operator without specifying values. Kubernetes supports several such operators, and the official documentation provides the full list.
When creating pods, these affinity rules are considered during scheduling. But what happens if no node matches the specified rules? Or what happens if the labels on a node change after pods are already running? This is determined by the type of node affinity. Currently, there are two available types:

* RequiredDuringSchedulingIgnoredDuringExecution: The scheduler must find a matching node when the pod is created; otherwise, the pod will not be scheduled. Once running, changes to labels are ignored.
* PreferredDuringSchedulingIgnoredDuringExecution: The scheduler tries to place the pod on a matching node, but if none is found, it will fall back to any available node. Again, changes after scheduling do not affect running pods.

Future Kubernetes releases plan to add RequiredDuringSchedulingRequiredDuringExecution, which means that if node labels change and a pod no longer satisfies the affinity rules, the pod will be evicted from the node. For now, however, all available options ignore affinity changes during execution, meaning pods continue to run even if labels are removed or modified.
That concludes our discussion on node affinity. Be sure to practice writing pod specifications with affinity rules in the coding exercises. In the next lecture, we will compare taints and tolerations with node affinity to see how they differ and when to use each.

# Taints & Tolerations vs Node Affinity
In this lecture, we will tie together the concepts of taints and tolerations and node affinity through a simple exercise. Imagine we have three nodes and three pods, each identified by a color: blue, red, and green. Our goal is to place each pod on the node of the same colorâ€”the blue pod on the blue node, the red pod on the red node, and the green pod on the green node. Since we are sharing the Kubernetes cluster with other teams, there are additional pods and nodes, and we want to ensure two things: first, that no external pods are placed on our dedicated nodes, and second, that our pods are not scheduled on other teamsâ€™ nodes.

We first attempt to solve this using taints and tolerations. By applying taints to the nodes with their respective colors and setting tolerations on the pods, we ensure that nodes only accept pods with matching tolerations. This correctly places the blue pod on the blue node and the green pod on the green node. However, taints and tolerations alone do not guarantee that a pod will always prefer its intended node, so the red pod may still end up on a different untainted node. This is not the behavior we want.

Next, we try solving it with node affinity. We label each node with its corresponding color and then set node selectors on the pods. This ensures that each pod is scheduled on the correct nodeâ€”for example, the red pod on the red node. But node affinity does not prevent other unrelated pods from being scheduled on these nodes, which again is not desirable.

The complete solution lies in combining taints and tolerations with node affinity. Taints and tolerations prevent other pods from being scheduled on our dedicated nodes, while node affinity ensures that our pods are only scheduled on their respective nodes. Together, these mechanisms allow us to fully dedicate nodes to specific pods, achieving the desired isolation and control.

That concludes this lecture. Be sure to head over to the coding exercises and practice working with taints, tolerations, and node affinity to solidify your understanding.

# Certification Tips - Student Tips
Make sure you check out these tips and tricks from other students who have cleared the exam:

https://www.linkedin.com/pulse/my-ckad-exam-experience-atharva-chauthaiwale/
https://medium.com/@harioverhere/ckad-certified-kubernetes-application-developer-my-journey-3afb0901014
https://github.com/lucassha/CKAD-resources

