1. Core Concept
================

The course begins with core concepts, reviewing Kubernetes architecture, pods, and essential configurations to refresh and reinforce foundational knowledge. Next, we dive into configuration—mastering ConfigMaps, security context, resource requirements, secrets, and service accounts to manage app configurations securely and efficiently. The multi-container pods module covers multi-container patterns like ambassador, adapter, or sidecar, with examples to demonstrate each use case. We then cover probes, monitoring, logging, and debugging, key skills to keep applications healthy and resolve issues. This includes setting up readiness and liveness probes for proactive monitoring. In workloads and scheduling, we explore labels, selectors, rolling updates, and rollbacks. We then dive into scheduling jobs and Cron Jobs for automated task management. The networking module covers services and network policies to manage connectivity and security within clusters. Finally, we cover persistent storage with persistent volumes and claims to ensure data durability across containers.

# Recap - Kubernetes Architecture
Hello and welcome to this lecture. Let’s begin with nodes. A node is a physical or virtual machine where Kubernetes is installed. Nodes are also called worker machines, and this is where Kubernetes launches and runs containers. In the past, nodes were referred to as minions, so you might occasionally see that term in older documentation.

But what happens if the node running your application fails? Naturally, the application goes down. To avoid this, you need more than one node. A group of nodes working together forms a cluster. Clustering ensures high availability—if one node fails, the application can still be served from another. Multiple nodes also allow Kubernetes to distribute and balance workloads more efficiently.

Now that we have a cluster, the next question is: who manages it? Where is cluster information stored? How are nodes monitored? And if a node fails, how is its workload moved elsewhere? This is the responsibility of the master node. The master is another machine with Kubernetes installed, but configured as a controller of the cluster. It watches over worker nodes and orchestrates containers across them.

When you install Kubernetes on any system, several components are deployed:

* API Server – the entry point to the cluster. All users, tools, and interfaces interact with Kubernetes through the API server.
* etcd – a distributed, reliable key-value store that maintains all cluster data, such as node information, configuration, and state. Etcd also manages locks to prevent conflicts between multiple masters.
* Scheduler – assigns containers to nodes by evaluating available resources and scheduling newly created pods accordingly.
* Controllers – the “brains” of orchestration, continuously monitoring cluster health. If nodes, pods, or endpoints fail, controllers respond by creating replacements.
* Container Runtime – the software that actually runs containers. While Docker is the most common runtime, alternatives like rkt or CRI-O also work.
* Kubelet – an agent running on every node, responsible for ensuring that containers are running as expected and for reporting back to the master.

So far, we’ve seen the two main server types: master and worker, and the components that make up Kubernetes. The difference between them lies in the roles of these components. Worker nodes (formerly minions) host containers, so they primarily run the container runtime (like Docker) and the kubelet. Master nodes, on the other hand, run the API server, etcd, the controller manager, and the scheduler. This division ensures clear responsibilities: masters manage, workers execute.

Finally, let’s introduce kubectl (pronounced kube-control), the command-line tool used to interact with Kubernetes. With kubectl, you can deploy and manage applications, inspect the cluster, monitor node status, and much more. For example:

* kubectl run – deploys an application to the cluster.
* kubectl cluster-info – displays information about the cluster.
* kubectl get nodes – lists all the nodes in the cluster.

As we progress through this course, we’ll continue to learn new kubectl commands alongside new concepts, applying them in practice. That’s all for now. See you in the next lecture.


# Docker-vs-ContainerD
As we move forward, you’ll often see references to both Docker and containerd. When reading older blogs or documentation, Docker is usually mentioned alongside Kubernetes, while newer resources often talk about containerd. This can raise questions about the difference between the two, and also about CLI tools like ctr, crictl, and nerdctl—which ones exist, and which ones should you use? To answer that, let’s step back to the early days of containers. Back then, Docker was the most popular tool because it made working with containers simple, even though alternatives like Rocket existed. Kubernetes itself was originally built to orchestrate Docker specifically, meaning Kubernetes and Docker were tightly coupled. As Kubernetes gained popularity, other runtimes like Rocket wanted compatibility, so the Kubernetes team introduced the Container Runtime Interface (CRI)—a standard way for Kubernetes to interact with any runtime that followed OCI (Open Container Initiative) standards. OCI defined two key things: imagespec (how container images should be built) and runtimespec (how runtimes should be developed). With these standards, any vendor could build a runtime that Kubernetes could use.

Docker, however, predated the CRI and wasn’t built with it in mind, so Kubernetes had to support Docker separately through a temporary compatibility layer called dockershim. While most runtimes plugged into Kubernetes via CRI, Docker bypassed it through dockershim. Over time, this extra maintenance became unnecessary, so Kubernetes decided to remove dockershim in version 1.24, dropping direct support for Docker as a runtime. Importantly, Docker images continued to work because they already followed OCI standards, meaning they remained compatible with containerd and other runtimes.

This brings us to containerd, which was originally part of Docker but later became its own CNCF project with graduated status. Containerd is CRI-compatible and can run directly with Kubernetes without Docker. You can even install containerd independently if you don’t need Docker’s extra features. But this raises another question: if Docker isn’t installed, how do you run containers with containerd alone? Containerd ships with a CLI called ctr, but ctr is primarily for debugging, offering only a limited set of commands like pulling images or running basic containers—not ideal for everyday use. For that, the community created nerdctl, a Docker-like CLI for containerd. Nerdctl mirrors most Docker commands and adds features like encrypted images, lazy pulling, peer-to-peer distribution, signing and verification, and namespace support—all things Docker CLI doesn’t support. In practice, you can often just replace docker with nerdctl in your commands.

Then there’s crictl, developed by the Kubernetes community. Unlike ctr or nerdctl, which are containerd-specific, crictl is designed to interact with any CRI-compatible runtime from Kubernetes’ perspective. It’s mostly used for debugging and troubleshooting at the Kubernetes node level. Commands like crictl ps, crictl logs, or crictl exec are very similar to their Docker equivalents, but crictl also understands pods, which Docker never did. However, crictl isn’t meant for general-purpose container management—if you try to create containers with it, kubelet may delete them since they’re outside its awareness. With Kubernetes 1.24, crictl’s default runtime endpoint was updated from dockershim.sock to cri-dockerd.sock, and users are now encouraged to set endpoints explicitly.

To summarize: ctr is a low-level debugging tool for containerd, rarely used in daily workflows. nerdctl is the practical, Docker-like CLI for working with containerd, making it the best choice for general-purpose container management. crictl is the Kubernetes community’s tool for debugging across all CRI-compatible runtimes. Together, these tools clarify the post-Docker world: while Docker is no longer required by Kubernetes, its legacy remains in the tools and standards that still shape container workflows today.


# A note of docker deprication
Every time we mention Docker in this course, a common question arises: why are we still talking about Docker if it’s deprecated? This confusion is understandable, so let’s clarify it. Docker was the original and only supported container runtime for Kubernetes. Later, to make Kubernetes more flexible and open to other runtimes, the Container Runtime Interface (CRI) was introduced. Docker itself is not just a runtime—it is a collection of tools, including the Docker CLI, the API, build tools for creating images, support for volumes, authentication, security, and most importantly, the container runtime called runc along with its managing daemon, containerd. Among these, containerd is CRI-compatible and can work directly with Kubernetes, just like other runtimes. Once this change was made, Kubernetes no longer needed Docker-specific tools such as the Docker CLI, API, or build utilities, because Kubernetes already provided those functions natively. That’s why Kubernetes officially deprecated direct support for Docker as a runtime.

However, this doesn’t mean Docker is gone. Docker remains the most popular container solution and is widely used for development and image building. The key change is that Kubernetes no longer requires Docker as the runtime, since it can work directly with containerd or other CRI-compliant runtimes. In this course, whenever we use Docker, it’s mainly as an example to help explain container concepts before transitioning to Kubernetes orchestration. This approach is perfectly fine, since Docker makes it easy to learn container basics. If you don’t have Docker installed and are only using containerd, you can usually adapt the same examples by replacing Docker commands with equivalent containerd or nerdctl commands.

So, while Docker support has been deprecated within Kubernetes itself, Docker as a tool is still very much relevant, especially for learning and development. I just wanted to make that clear before we move ahead.


# Recap - pods
Hello and welcome to this lecture on Kubernetes Pods. Before we dive into understanding Pods, let’s assume a few prerequisites are already in place. First, the application has been developed, built into Docker images, and pushed to a Docker repository such as Docker Hub so that Kubernetes can pull it down. Second, the Kubernetes cluster has already been set up and is running—this could be a single-node or a multi-node cluster, as long as all the services are in a healthy state.

As we discussed earlier, Kubernetes aims to deploy applications in the form of containers across worker nodes in a cluster. However, Kubernetes does not deploy containers directly; instead, containers are encapsulated into an object called a Pod, which is the smallest deployable unit in Kubernetes and typically represents a single instance of an application. For example, in a simple scenario with a single-node Kubernetes cluster, you may have one Pod running a single container with your application. If the number of users grows and the application needs to scale, you don’t add more containers to the same Pod; instead, you create additional Pods, each running its own container instance. Kubernetes can spread these Pods across the same node or across new nodes as your cluster scales out, allowing you to handle increased load. Scaling up means creating new Pods, while scaling down means deleting existing Pods.

Now, while Pods usually have a one-to-one relationship with containers, Kubernetes also allows multiple containers inside a single Pod. This is not meant for scaling the same application container, but rather for scenarios where you need a helper container to support your main application—for example, processing user data or handling background tasks. In such cases, the main application container and the helper container live inside the same Pod, sharing the same network and storage, and following the same lifecycle. They can communicate via localhost and are created and destroyed together. This setup makes managing tightly coupled containers much easier.

To better understand Pods, let’s compare them with plain Docker. Without Kubernetes, if you deploy your application with docker run, scaling means running more docker run commands. Later, if your application architecture grows more complex and requires helper containers, you would have to manually manage container mappings, networks, volumes, and dependencies. For example, you would need to set up custom networks to enable communication, manage volume sharing, and monitor containers so that if the main container dies, its helper container is also terminated. Kubernetes Pods handle all of this automatically. By defining the containers inside a Pod, Kubernetes ensures they share storage, networking, and lifecycle—removing the need for manual container management. Even if your application only needs one container today, Pods future-proof it for scaling and architectural changes later. However, note that multi-container Pods are relatively rare, and in most cases, we stick to one container per Pod.

Now let’s see how to deploy Pods. The kubectl run command is used to deploy a Docker container by first creating a Pod for it. For example, kubectl run nginx --image=nginx creates a Pod and deploys an instance of the nginx Docker image, which is pulled from Docker Hub by default unless configured to use a private registry. Once deployed, you can view Pods with the kubectl get pods command. Initially, the Pod status may show as ContainerCreating, and once the container is running, it will switch to Running. At this point, the Pod exists, but external users cannot access the NGINX web server yet because we haven’t configured networking or services. For now, you can only access it internally from the node. In a later lecture, we’ll learn how to expose Pods to external users with Services.


# YAML BASIC 
Going forward, we will be working with YAML files to create and configure Kubernetes objects. If you are new to YAML, please refer to the YAML basics and exercises in the Kubernetes for the Absolute Beginners course. In this lecture, we will focus on creating a pod using a YAML-based configuration file. Previously, we discussed YAML files in general; now, we will see how they are specifically used in Kubernetes. Kubernetes relies on YAML files as input for creating objects such as Pods, ReplicaSets, Deployments, and Services. All of these follow a similar structure, where a definition file always contains four top-level fields: apiVersion, kind, metadata, and spec. These are required fields in every configuration file.

The apiVersion specifies which version of the Kubernetes API is being used. Depending on the object being created, the value changes—for Pods, we set it to v1, while for others it could be apps/v1, extensions/v1beta, and so on. The kind field indicates the type of object to create, such as Pod, ReplicaSet, Deployment, or Service. Next is metadata, which contains information about the object, like its name and labels. Unlike the first two fields that take string values, metadata is a dictionary, meaning its properties must be indented. Proper indentation is critical: name and labels should be siblings under metadata and indented equally, otherwise the YAML structure becomes invalid. Under metadata, name is a string (e.g., myapp-pod), while labels is another dictionary where you can define key-value pairs (e.g., app: myapp). Labels are especially useful when managing many Pods, as they let you group and filter Pods later by roles like frontend, backend, or database.

The final section, spec, defines the details of the object. This part varies depending on the Kubernetes resource. For Pods, it includes the containers property, which is a list, since a Pod can run multiple containers. Each container is represented as an item in the list, starting with a dash. In this example, we define a single container with two properties: name and image. The container name can be arbitrary, while the image is set to nginx, which refers to the Docker image in the repository. Once the YAML file is created, you can use kubectl create -f pod-definition.yaml to create the Pod. To verify, run kubectl get pods to see the list of Pods, and kubectl describe pod <pod-name> for detailed information such as creation time, assigned labels, container details, and associated events.

In summary, every Kubernetes YAML definition begins with the four root fields: apiVersion, kind, metadata, and spec. From there, you fill in the details depending on the object you want to create. That concludes this lecture; next, we will move on to a demo.


# Recap - Demo - Creating Pods with YAML
Hello, and welcome to this demo. In this session, we’re going to see how to create a pod using a YAML definition file. To create YAML files, you can use any editor of your choice, such as Notepad, Notepad++, Atom, or others. Personally, I prefer PyCharm, a JetBrains IDE designed for Python, which also has good support for YAML files. If you’d like to use PyCharm, you can download it from jetbrains.com/pycharm. There are two versions available: the professional edition and the free community edition, which is lightweight and suitable for working with definition files. PyCharm also supports helpful plugins for validating YAML files, making the process easier.

To start, I created a new project called Pod, and inside it, I added a file named pod-definition.yaml. As we’ve discussed earlier, every Kubernetes definition file has four root-level properties: apiVersion, kind, metadata, and spec. For a pod, the apiVersion is set to v1, and the kind is Pod. Under metadata, I provided a name property with the value myapp-pod, and then defined labels. Labels are key-value pairs used for grouping pods; for example, app: MyApp. Proper indentation is critical in YAML, as it reflects hierarchy. For instance, name and labels should be siblings under metadata, while keys under labels should be indented further.

Next, under the spec section, I defined containers. Since containers is a list, each entry starts with a dash (-). I added one container with the name nginx-container and the image nginx. You can add more containers by adding additional list items, but for now, I kept only one. After saving this file, I moved to the Kubernetes master node, created a folder structure for my demos, and added the pod-definition.yaml file under the Pod directory.

Before creating the pod, I verified that no pods were running using kubectl get pods. Then, I created the pod with kubectl create -f pod-definition.yaml. Once created, I ran kubectl get pods again to confirm the pod’s status. Initially, it showed as “ContainerCreating,” and shortly after, it switched to “Running,” confirming that the pod was successfully deployed.

That concludes this demo. In the next session, we’ll explore some tips and tricks for working with YAML files in PyCharm. Thank you for your time, and see you in the next demo.

# 16. NOTES
The previous lecture was for informational purposes only. In the real exam, you will not be able to use PyCharm. You will need to use an editor available in Linux such as vi or nano. When working on labs, practice working with one of these editors. We demo how to work with vi editor in the solution videos of the labs.

# edit pods
Edit Pods
A Note on Editing Existing Pods
In any of the practical quizzes, if you are asked to edit an existing POD, please note the following:
If you are given a pod definition file, edit that file and use it to create a new pod.
If you are not given a pod definition file, you may extract the definition to a file using the below command:
kubectl get pod <pod-name> -o yaml > pod-definition.yaml
Then edit the file to make the necessary changes, delete, and re-create the pod.
To modify the properties of the pod, you can utilize the kubectl edit pod <pod-name> command. Please note that only the properties listed below are editable.
spec.containers[*].image
spec.initContainers[*].image
spec.activeDeadlineSeconds
spec.tolerations

# Recap - ReplicaSets
Hello, and welcome to this lecture on Kubernetes controllers. My name is Mumshad Mannambeth, and in this session we will focus on one specific controller: the ReplicationController. Controllers are the brain behind Kubernetes—they constantly monitor objects and respond when changes are needed. The ReplicationController ensures that the desired number of pod replicas are running at all times, providing both high availability and load distribution.

Consider a scenario where we run only a single pod for our application. If that pod crashes, the application becomes unavailable. With a ReplicationController, we can maintain multiple pod instances so that even if one fails, the others continue serving users. Even if we configure it for just a single pod, the ReplicationController automatically creates a replacement if the pod fails. In addition to reliability, it helps scale applications by creating multiple pods across different nodes, distributing load as demand increases.

It’s important to distinguish between ReplicationControllers and ReplicaSets. Both serve the same purpose, but ReplicaSet is the newer, recommended approach, whereas ReplicationController is the older technology being phased out. The concepts we learn apply to both, though ReplicaSet introduces additional features such as label selectors.

To create a ReplicationController, we define it in a YAML file, typically named rc-definition.yaml. Like all Kubernetes definition files, it has four key sections: apiVersion, kind, metadata, and spec. For ReplicationController, the API version is v1, and the kind is ReplicationController. Under metadata, we specify the name (e.g., myapp-rc) and labels. The spec section is crucial: it defines the pod template that the ReplicationController uses to create replicas. This pod template looks just like the pod definition file we wrote earlier, but is nested inside the controller’s spec. We also specify the number of replicas we want. Once ready, we create the controller with kubectl create -f rc-definition.yaml. Kubernetes will then create the specified number of pods, which can be verified with kubectl get replicationcontroller and kubectl get pods.

Now let’s look at ReplicaSet, which is very similar in structure. Its API version is apps/v1, and the kind is ReplicaSet. The metadata and spec sections follow the same pattern, with replicas and a pod template. However, ReplicaSet requires a selector section. The selector, often written as matchLabels, tells the ReplicaSet which pods it should manage. This allows a ReplicaSet to adopt pods that were created outside of it, as long as their labels match. ReplicationController assumes selectors automatically if not provided, but ReplicaSet makes it mandatory.

For example, suppose we already have three frontend pods running with the label app: frontend. If we create a ReplicaSet with a selector matching that label and set replicas to three, it will adopt and manage those existing pods. If one fails, the ReplicaSet will create a replacement using the template definition we provide. This is why a template section is always required, even if pods already exist.

Scaling a ReplicaSet can be done in multiple ways. We can edit the definition file, update the replicas field, and run kubectl replace -f filename.yaml. Alternatively, we can use kubectl scale with the --replicas flag, either referencing the file or the ReplicaSet name directly. Note, however, that scaling via command line does not update the replicas count in the YAML file itself. Kubernetes also supports autoscaling based on load, though that is an advanced topic for later.

To summarize: use kubectl create -f to create ReplicaSets, kubectl get replicaset to list them, kubectl delete replicaset to remove them, kubectl replace -f to update them, and kubectl scale to adjust replica counts dynamically. ReplicaSets, together with labeling and selectors, provide powerful mechanisms to ensure high availability, balance workloads, and scale applications seamlessly within Kubernetes.

# recap - deployment
Hello, and welcome to this lecture. My name is Munshat Manambat, and in this session we will discuss Kubernetes Deployments. To understand deployments, let us step away for a moment from pods, replica sets, and other Kubernetes concepts, and think about how we would deploy applications in a production environment. For instance, if you have a web server to deploy, you usually need multiple instances running for reliability. When a new version of the application becomes available in the Docker registry, you want to upgrade your running instances without downtime. However, upgrading all instances at once could disrupt users, so Kubernetes allows rolling updates, where pods are updated gradually. If a new upgrade causes unexpected issues, Kubernetes also provides the ability to roll back to the previous version. Additionally, you might want to make multiple changes—such as upgrading versions, scaling replicas, or adjusting resources—and apply them all together. Deployments support pausing and resuming updates so that these changes can be rolled out in a controlled manner.

Up to now, we have seen pods, which deploy single instances of applications, and replica sets or replication controllers, which manage multiple pods. Deployments build on top of these by introducing powerful lifecycle management features. A deployment is a higher-level Kubernetes object that manages replica sets and, by extension, the pods themselves. With deployments, you gain the ability to perform rolling updates, rollbacks, and batch configuration changes.

To create a deployment, we define it in a YAML file, much like other Kubernetes objects. The structure is almost identical to a ReplicaSet definition, except that the kind is set to Deployment. The definition contains the apiVersion (which is apps/v1), metadata with a name and labels, and a spec that includes replicas, a selector, and a pod template. The pod template defines the containers to run. Once the definition is ready, we use the command kubectl create -f deployment-definition.yaml to create the deployment. We can then verify it with kubectl get deployments. Behind the scenes, the deployment automatically creates a ReplicaSet, which you can see using kubectl get rs. That ReplicaSet, in turn, manages the actual pods, visible with kubectl get pods.

At this stage, the difference between a ReplicaSet and a Deployment may seem minimal—except that the deployment introduces this new higher-level object. But as we move forward, we will explore how deployments bring real advantages through rolling updates, rollbacks, and controlled configuration changes. Finally, to view all related resources at once, you can run kubectl get all. You’ll see the deployment itself, the associated ReplicaSet, and the pods created as part of that deployment.

That concludes this lecture.

# Certification Tip: Formatting Output with kubectl                                                                   
The default output format for all kubectl commands is the human-readable plain-text format.
The -o flag allows us to output the details in several different formats.
kubectl [command] [TYPE] [NAME] -o <output_format>
Here are some of the commonly used formats:

-o jsonOutput a JSON formatted API object.
-o namePrint only the resource name and nothing else.
-o wideOutput in the plain-text format with any additional information.
-o yamlOutput a YAML formatted API object.
Here are some useful examples:
Output with JSON format:
master $ kubectl create namespace test-123 --dry-run -o json
{
    "kind": "Namespace",
    "apiVersion": "v1",
    "metadata": {
        "name": "test-123",
        "creationTimestamp": null
    },
    "spec": {},
    "status": {}
}
master $

Output with YAML format:
master $ kubectl create namespace test-123 --dry-run -o yaml
apiVersion: v1
kind: Namespace
metadata:
  creationTimestamp: null
  name: test-123
spec: {}
status: {}

Output with wide (additional details):
Probably the most common format used to print additional details about the object:
master $ kubectl get pods -o wide
NAME      READY   STATUS    RESTARTS   AGE     IP          NODE     NOMINATED NODE   READINESS GATES
busybox   1/1     Running   0          3m39s   10.36.0.2   node01   <none>           <none>
ningx     1/1     Running   0          7m32s   10.44.0.1   node03   <none>           <none>
redis     1/1     Running   0          3m59s   10.36.0.1   node01   <none>           <none>
master $

For more details, refer:
https://kubernetes.io/docs/reference/kubectl/overview/
https://kubernetes.io/docs/reference/kubectl/cheatsheet/

# recap - namespace
Hello, and welcome to this lecture. In this session, we will discuss namespaces in Kubernetes. To begin, consider an analogy: there are two boys named Mark, distinguished by their last names—Smith and Williams—coming from different houses. Each house has its own set of rules and resources, and individuals within a house address each other by first names, while outsiders use full names to differentiate them. In Kubernetes, these “houses” correspond to namespaces.

So far, the objects we have created, such as Pods, Deployments, and Services, have been created within the default namespace, which Kubernetes sets up automatically. Additionally, Kubernetes creates a kube-system namespace for internal resources such as networking and DNS services, and a kube-public namespace for resources that should be accessible to all users. While small or learning environments may not require multiple namespaces, in enterprise or production setups, namespaces help isolate resources. For example, separate namespaces can be used for development and production environments, ensuring that changes in one do not affect the other. Namespaces can also have resource quotas to limit consumption, guaranteeing fair use of cluster resources.

Within a namespace, objects can reference each other simply by name, similar to how individuals within a house refer to each other by first names. For example, a web app Pod can reach a database service using just its name. To reference resources in another namespace, you must append the namespace to the service name using the format service-name.namespace.svc.cluster.local. Here, cluster.local is the default cluster domain, svc is the subdomain for services, followed by the namespace and the service name.

From an operational standpoint, kubectl commands default to the current namespace. For instance, kubectl get pods lists Pods in the default namespace. To view Pods in another namespace, use the --namespace flag. You can also specify a namespace directly in the object’s YAML file under metadata to ensure resources are always created in the correct namespace. New namespaces can be created either via a YAML definition file or directly with the command kubectl create namespace <name>.

If you work in multiple namespaces, you can switch the current namespace in your context using kubectl config set-context --current --namespace=<name>. This avoids repeatedly specifying the namespace flag. To view resources across all namespaces, use the --all-namespaces option. Kubernetes contexts allow management of multiple clusters and environments, which is a more advanced topic discussed separately.

Finally, to limit resources in a namespace, you can create a ResourceQuota object. A YAML definition specifies the namespace and sets limits such as the number of Pods, CPU units, or memory allocation. This ensures fair resource distribution and prevents any namespace from over-consuming resources.

Namespaces are a powerful way to organize, isolate, and manage resources in Kubernetes clusters. Head over to the coding exercises to practice working with namespaces, and we’ll continue building on these concepts in the next lecture.

# Certification Tip: Imperative Commands
While you would be working mostly the declarative way - using definition files, imperative commands can help in getting one-time tasks done quickly, as well as generate a definition template easily. This would help save a considerable amount of time during your exams.
Before we begin, familiarize yourself with the two options that can come in handy while working with the below commands:

--dry-run: By default, as soon as the command is run, the resource will be created. If you simply want to test your command, use the --dry-run=client option. This will not create the resource. Instead, tell you whether the resource can be created and if your command is right.
-o yaml: This will output the resource definition in YAML format on the screen.

Use the above two in combination along with Linux output redirection to generate a resource definition file quickly, that you can then modify and create resources as required, instead of creating the files from scratch.
* kubectl run nginx --image=nginx --dry-run=client -o yaml > nginx-pod.yaml

POD
====
Create an NGINX Pod = kubectl run nginx --image=nginx
Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run) = kubectl run nginx --image=nginx --dry-run=client -o yaml

Deployment
===========
Create a deployment = kubectl create deployment --image=nginx nginx
Generate Deployment YAML file (-o yaml). Don't create it(--dry-run) = kubectl create deployment --image=nginx nginx --dry-run -o yaml
Generate Deployment with 4 Replicas = kubectl create deployment nginx --image=nginx --replicas=4
You can also scale deployment using the kubectl scale command. = kubectl scale deployment nginx --replicas=4

Another way to do this is to save the YAML definition to a file and modify
kubectl create deployment nginx --image=nginx--dry-run=client -o yaml > nginx-deployment.yaml

You can then update the YAML file with the replicas or any other field before creating the deployment.

Service
=======
Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379
kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml
(This will automatically use the pod's labels as selectors)
Or
kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml
 (This will not use the pods' labels as selectors; instead it will assume selectors as app=redis. You cannot pass in selectors as an option. So it does not work well if your pod has a different label set. So generate the file and modify the selectors before creating the service)

Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port 30080 on the nodes:
kubectl expose pod nginx --port=80 --name nginx-service --type=NodePort --dry-run=client -o yaml
(This will automatically use the pod's labels as selectors, but you cannot specify the node port. You have to generate a definition file and then add the node port in manually before creating the service with the pod.)
Or
kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml
(This will not use the pods' labels as selectors)
Both the above commands have their own challenges. While one of it cannot accept a selector the other cannot accept a node port. I would recommend going with the `kubectl expose` command. If you need to specify a node port, generate a definition file using the same command and manually input the nodeport before creating the service.

Reference:
https://kubernetes.io/docs/reference/kubectl/conventions/

# 